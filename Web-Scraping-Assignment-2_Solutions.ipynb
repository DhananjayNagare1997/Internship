{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ec408c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/97/e3/fd7272d6d2c49fd49a79a603cb28c8b5a71f8911861b4a0409b3c006a241/selenium-4.17.2-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.17.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/14/fb/9299cf74953f473a15accfdbe2c15218e766bae8c796f2567c83bae03e98/trio-0.24.0-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.24.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium)\n",
      "  Obtaining dependency information for typing_extensions>=4.9.0 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\rohit\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/58.3 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/58.3 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 41.0/58.3 kB 245.8 kB/s eta 0:00:01\n",
      "     -------------------------- ----------- 41.0/58.3 kB 245.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 58.3/58.3 kB 219.3 kB/s eta 0:00:00\n",
      "Downloading selenium-4.17.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.9 MB 465.5 kB/s eta 0:00:22\n",
      "   ---------------------------------------- 0.1/9.9 MB 465.5 kB/s eta 0:00:22\n",
      "   ---------------------------------------- 0.1/9.9 MB 403.5 kB/s eta 0:00:25\n",
      "   ---------------------------------------- 0.1/9.9 MB 403.5 kB/s eta 0:00:25\n",
      "   ---------------------------------------- 0.1/9.9 MB 327.2 kB/s eta 0:00:30\n",
      "   ---------------------------------------- 0.1/9.9 MB 327.2 kB/s eta 0:00:30\n",
      "    --------------------------------------- 0.2/9.9 MB 339.7 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.2/9.9 MB 339.7 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.2/9.9 MB 317.5 kB/s eta 0:00:31\n",
      "    --------------------------------------- 0.2/9.9 MB 317.5 kB/s eta 0:00:31\n",
      "    --------------------------------------- 0.2/9.9 MB 317.5 kB/s eta 0:00:31\n",
      "    --------------------------------------- 0.2/9.9 MB 335.5 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.2/9.9 MB 335.5 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.2/9.9 MB 335.5 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.2/9.9 MB 335.5 kB/s eta 0:00:29\n",
      "   - -------------------------------------- 0.3/9.9 MB 275.8 kB/s eta 0:00:35\n",
      "   - -------------------------------------- 0.3/9.9 MB 288.8 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.3/9.9 MB 288.8 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.3/9.9 MB 288.8 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.3/9.9 MB 279.4 kB/s eta 0:00:35\n",
      "   - -------------------------------------- 0.3/9.9 MB 279.4 kB/s eta 0:00:35\n",
      "   - -------------------------------------- 0.3/9.9 MB 283.2 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.3/9.9 MB 283.2 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.4/9.9 MB 286.7 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.4/9.9 MB 286.7 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.4/9.9 MB 286.1 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.4/9.9 MB 286.1 kB/s eta 0:00:34\n",
      "   - -------------------------------------- 0.5/9.9 MB 303.0 kB/s eta 0:00:32\n",
      "   - -------------------------------------- 0.5/9.9 MB 303.0 kB/s eta 0:00:32\n",
      "   - -------------------------------------- 0.5/9.9 MB 304.5 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.5/9.9 MB 304.5 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.5/9.9 MB 304.5 kB/s eta 0:00:31\n",
      "   -- ------------------------------------- 0.5/9.9 MB 300.1 kB/s eta 0:00:32\n",
      "   -- ------------------------------------- 0.5/9.9 MB 301.5 kB/s eta 0:00:32\n",
      "   -- ------------------------------------- 0.6/9.9 MB 307.7 kB/s eta 0:00:31\n",
      "   -- ------------------------------------- 0.6/9.9 MB 307.7 kB/s eta 0:00:31\n",
      "   -- ------------------------------------- 0.6/9.9 MB 313.8 kB/s eta 0:00:30\n",
      "   -- ------------------------------------- 0.6/9.9 MB 316.8 kB/s eta 0:00:30\n",
      "   -- ------------------------------------- 0.6/9.9 MB 319.8 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 0.6/9.9 MB 319.8 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 0.7/9.9 MB 320.3 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 0.7/9.9 MB 325.2 kB/s eta 0:00:29\n",
      "   -- ------------------------------------- 0.7/9.9 MB 330.0 kB/s eta 0:00:28\n",
      "   -- ------------------------------------- 0.7/9.9 MB 330.0 kB/s eta 0:00:28\n",
      "   --- ------------------------------------ 0.8/9.9 MB 343.4 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/9.9 MB 343.4 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/9.9 MB 340.6 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/9.9 MB 338.2 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.8/9.9 MB 342.5 kB/s eta 0:00:27\n",
      "   --- ------------------------------------ 0.9/9.9 MB 350.0 kB/s eta 0:00:26\n",
      "   --- ------------------------------------ 0.9/9.9 MB 350.0 kB/s eta 0:00:26\n",
      "   --- ------------------------------------ 0.9/9.9 MB 361.5 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 1.0/9.9 MB 360.9 kB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 367.9 kB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 1.0/9.9 MB 374.4 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 382.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 383.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 383.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.1/9.9 MB 383.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 384.2 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 385.0 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 387.2 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 388.0 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 388.0 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 389.3 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 393.8 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 397.5 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 398.1 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 1.4/9.9 MB 402.1 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 1.4/9.9 MB 403.3 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 1.4/9.9 MB 406.7 kB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 1.5/9.9 MB 411.0 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 1.5/9.9 MB 411.6 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 1.5/9.9 MB 415.5 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 1.6/9.9 MB 418.5 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 1.6/9.9 MB 419.7 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 1.6/9.9 MB 422.7 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 1.7/9.9 MB 425.5 kB/s eta 0:00:20\n",
      "   ------ --------------------------------- 1.7/9.9 MB 428.2 kB/s eta 0:00:20\n",
      "   ------- -------------------------------- 1.7/9.9 MB 436.0 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 1.8/9.9 MB 436.8 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 1.8/9.9 MB 443.6 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 1.9/9.9 MB 451.8 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 1.9/9.9 MB 455.7 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 2.0/9.9 MB 466.0 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.0/9.9 MB 466.0 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 2.0/9.9 MB 466.0 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.0/9.9 MB 465.0 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.1/9.9 MB 467.0 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.1/9.9 MB 475.3 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.2/9.9 MB 487.0 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.3/9.9 MB 493.2 kB/s eta 0:00:16\n",
      "   --------- ------------------------------ 2.3/9.9 MB 504.2 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 2.4/9.9 MB 511.9 kB/s eta 0:00:15\n",
      "   --------- ------------------------------ 2.4/9.9 MB 519.9 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 2.5/9.9 MB 527.7 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 543.0 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 2.7/9.9 MB 552.4 kB/s eta 0:00:14\n",
      "   ---------- ----------------------------- 2.7/9.9 MB 552.4 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 555.8 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 555.3 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 2.8/9.9 MB 559.4 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 562.6 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 565.9 kB/s eta 0:00:13\n",
      "   ------------ --------------------------- 3.0/9.9 MB 576.7 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.1/9.9 MB 587.1 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.2/9.9 MB 597.3 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 3.2/9.9 MB 607.3 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 3.3/9.9 MB 609.6 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 3.4/9.9 MB 617.6 kB/s eta 0:00:11\n",
      "   ------------- -------------------------- 3.4/9.9 MB 625.4 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.5/9.9 MB 634.9 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.5/9.9 MB 635.0 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.5/9.9 MB 635.0 kB/s eta 0:00:11\n",
      "   -------------- ------------------------- 3.6/9.9 MB 633.6 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 3.6/9.9 MB 633.6 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 3.7/9.9 MB 634.0 kB/s eta 0:00:10\n",
      "   -------------- ------------------------- 3.7/9.9 MB 634.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.7/9.9 MB 634.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.7/9.9 MB 634.3 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.8/9.9 MB 633.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.8/9.9 MB 633.0 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.8/9.9 MB 628.4 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.9/9.9 MB 633.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.9/9.9 MB 633.5 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.9/9.9 MB 633.8 kB/s eta 0:00:10\n",
      "   --------------- ------------------------ 3.9/9.9 MB 633.8 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.0/9.9 MB 634.1 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.0/9.9 MB 634.1 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 634.5 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 634.5 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 633.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 633.2 kB/s eta 0:00:10\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 635.1 kB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 4.2/9.9 MB 635.1 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 636.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 636.9 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 631.2 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 637.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 637.3 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 637.6 kB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 634.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.5/9.9 MB 637.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.5/9.9 MB 637.8 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.6/9.9 MB 639.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.6/9.9 MB 639.5 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.6/9.9 MB 641.2 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.6/9.9 MB 641.2 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.7/9.9 MB 641.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.7/9.9 MB 641.3 kB/s eta 0:00:09\n",
      "   ------------------ --------------------- 4.7/9.9 MB 641.3 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 4.7/9.9 MB 636.0 kB/s eta 0:00:09\n",
      "   ------------------- -------------------- 4.8/9.9 MB 639.0 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 4.8/9.9 MB 641.7 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 4.9/9.9 MB 637.8 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 4.9/9.9 MB 643.3 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 4.9/9.9 MB 643.3 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.0/9.9 MB 643.4 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.0/9.9 MB 643.4 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.1/9.9 MB 644.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.1/9.9 MB 644.9 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.1/9.9 MB 647.6 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.1/9.9 MB 647.6 kB/s eta 0:00:08\n",
      "   -------------------- ------------------- 5.1/9.9 MB 647.6 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 5.2/9.9 MB 646.4 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 5.3/9.9 MB 647.8 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 5.3/9.9 MB 649.0 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 5.3/9.9 MB 649.0 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 5.4/9.9 MB 652.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 5.4/9.9 MB 652.8 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.5/9.9 MB 654.1 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.5/9.9 MB 654.1 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 656.5 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 656.5 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 5.6/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 657.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.9/9.9 MB 661.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.9/9.9 MB 661.1 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 5.9/9.9 MB 658.8 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 5.9/9.9 MB 661.0 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.0/9.9 MB 662.2 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.1/9.9 MB 664.4 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.1/9.9 MB 665.5 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.1/9.9 MB 669.8 kB/s eta 0:00:06\n",
      "   ------------------------ --------------- 6.2/9.9 MB 668.7 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 6.2/9.9 MB 670.8 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 6.3/9.9 MB 671.9 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 6.3/9.9 MB 676.1 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 6.4/9.9 MB 674.9 kB/s eta 0:00:06\n",
      "   ------------------------- -------------- 6.4/9.9 MB 672.6 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 6.4/9.9 MB 675.8 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 6.4/9.9 MB 675.8 kB/s eta 0:00:06\n",
      "   -------------------------- ------------- 6.5/9.9 MB 678.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 6.5/9.9 MB 678.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 6.6/9.9 MB 681.8 kB/s eta 0:00:05\n",
      "   -------------------------- ------------- 6.6/9.9 MB 681.7 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.7/9.9 MB 685.7 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.7/9.9 MB 685.6 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.8/9.9 MB 687.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.8/9.9 MB 687.4 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.9/9.9 MB 687.1 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 6.9/9.9 MB 688.0 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 6.9/9.9 MB 687.9 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 7.0/9.9 MB 689.7 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 7.1/9.9 MB 693.5 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 7.1/9.9 MB 693.5 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 7.1/9.9 MB 693.3 kB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 7.2/9.9 MB 695.1 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.2/9.9 MB 695.8 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 698.6 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 698.5 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 698.5 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 693.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 693.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 693.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.3/9.9 MB 693.2 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 687.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 687.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 687.7 kB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 687.7 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.5/9.9 MB 680.6 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.5/9.9 MB 680.6 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.5/9.9 MB 680.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.5/9.9 MB 680.4 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/9.9 MB 679.2 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/9.9 MB 679.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/9.9 MB 679.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/9.9 MB 679.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 7.6/9.9 MB 675.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.7/9.9 MB 674.3 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.7/9.9 MB 675.0 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.8/9.9 MB 676.8 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.9/9.9 MB 680.2 kB/s eta 0:00:04\n",
      "   ------------------------------- -------- 7.9/9.9 MB 683.7 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/9.9 MB 686.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/9.9 MB 686.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/9.9 MB 686.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.0/9.9 MB 683.2 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/9.9 MB 681.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/9.9 MB 681.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/9.9 MB 681.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/9.9 MB 678.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.1/9.9 MB 677.5 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.2/9.9 MB 678.3 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 8.2/9.9 MB 678.3 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.2/9.9 MB 674.7 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.3/9.9 MB 675.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.3/9.9 MB 675.4 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.3/9.9 MB 672.7 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/9.9 MB 671.8 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/9.9 MB 671.8 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 8.4/9.9 MB 671.8 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 670.8 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.4/9.9 MB 670.8 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 670.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 670.7 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 669.0 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.5/9.9 MB 669.0 kB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 668.1 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 668.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 667.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 667.2 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 666.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 666.3 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.7/9.9 MB 663.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.8/9.9 MB 663.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/9.9 MB 664.5 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 8.9/9.9 MB 664.5 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 8.9/9.9 MB 659.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.0/9.9 MB 660.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.0/9.9 MB 663.7 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.1/9.9 MB 662.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.1/9.9 MB 662.8 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 9.1/9.9 MB 662.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.2/9.9 MB 662.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.2/9.9 MB 660.5 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.2/9.9 MB 662.7 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.2/9.9 MB 662.7 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 9.3/9.9 MB 662.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 662.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 661.9 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 661.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 661.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 661.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 663.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 663.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/9.9 MB 663.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/9.9 MB 663.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/9.9 MB 664.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 665.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 665.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 667.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/9.9 MB 667.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 669.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 669.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 667.4 kB/s eta 0:00:00\n",
      "Downloading trio-0.24.0-py3-none-any.whl (460 kB)\n",
      "   ---------------------------------------- 0.0/460.2 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 71.7/460.2 kB 2.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 92.2/460.2 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 174.1/460.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 194.6/460.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 276.5/460.2 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 317.4/460.2 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 399.4/460.2 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 430.1/460.2 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 460.2/460.2 kB 993.3 kB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: typing_extensions, sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.17.2 sniffio-1.3.0 trio-0.24.0 trio-websocket-0.11.1 typing_extensions-4.9.0 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebf3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34f8f3ac",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Analyst” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfb8081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.shine.com/new/job-search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be3f4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation & location as required into the question.\n",
    "\n",
    "job_title_input = driver.find_element(By.XPATH, \"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "\n",
    "job_title_input.send_keys('Data Analyst')\n",
    "\n",
    "location_input = driver.find_element(By.XPATH, \"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "\n",
    "location_input.send_keys('Banglore')\n",
    "\n",
    "#Now inspect search button find the class name & click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[2]/div/button\")\n",
    "\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e6ff558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sleep for some duration so all the data can loaded\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aa221b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "\n",
    "company_name=[]\n",
    "\n",
    "experiance_required=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "226eaebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lead Data Analyst', 'Data Analyst', 'Vacancy For Data Analyst', 'Clinical Data Analyst', 'Data Management', 'Data Modeler data', 'Data Modeller', 'Data Engineer Ii', 'Full time Opportunity-Networking Advisor-Cisco Nexus -D ...', 'Data Modeler Bangalore']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Scraping Job title from the given page\n",
    "\n",
    "title_tags = driver.find_elements(By.XPATH, '//div[@class=\"parentClass position-relative\"]/div/div/div/h2/a')\n",
    "\n",
    "for i in title_tags[:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "# Output the scraped job titles\n",
    "print(job_title)\n",
    "\n",
    "print(len(job_title))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb47d7c0",
   "metadata": {},
   "source": [
    "# Scrape first 10 job titles\n",
    "for i in title_tags[:10]:\n",
    "    title = i.text\n",
    "    job_title.append(title)\n",
    "\n",
    "# Output the scraped job titles\n",
    "print(job_title)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a471c8cd",
   "metadata": {},
   "source": [
    "# Scraping Company Experience required\n",
    "experience_tags = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "for exp_tag in experience_tags:\n",
    "    exp = exp_tag.text\n",
    "    experiance_required.append(exp)\n",
    "    \n",
    "print(experiance_required)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87226a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4 to 9 Yrs', '0 to 1 Yr', '0 to 3 Yrs', '0 to 1 Yr', '15 to >25 Yrs', '3 to 6 Yrs', '3 to 6 Yrs', '0 to 1 Yr', '10 to 20 Yrs', '3 to 6 Yrs']\n"
     ]
    }
   ],
   "source": [
    "# Scraping Company Experience required\n",
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10\n",
    "experiance_required = []\n",
    "\n",
    "for page in range(start, end):\n",
    "    experience_tags = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]')\n",
    "    \n",
    "    for exp_tag in experience_tags:\n",
    "        exp = exp_tag.text\n",
    "        experiance_required.append(exp)\n",
    "        if len(experiance_required) >= desired_records:\n",
    "            break  # Break if desired number of records is reached\n",
    "    \n",
    "    if len(experiance_required) >= desired_records:\n",
    "        break  # Break if desired number of records is reached\n",
    "    \n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[1]/div/div[1]/div[6]/div[3]/ul/li[2]/span')\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "print(experiance_required[:desired_records]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a005bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Company name from the given page\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "for i in company_tags:\n",
    "\n",
    "    company = i.text\n",
    "\n",
    "    company_name.append(company)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a78e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10\n",
    "company_names = []\n",
    "\n",
    "for page in range(start, end):\n",
    "    company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "    for i in company_tags:\n",
    "        company_name = i.text\n",
    "        company_names.append(company_name)\n",
    "        if len(company_names) >= desired_records:\n",
    "            break  # Break if desired number of records is reached\n",
    "    \n",
    "    if len(company_names) >= desired_records:\n",
    "        break  # Break if desired number of records is reached\n",
    "\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[1]/div/div[1]/div[6]/div[3]/ul/li[2]/span')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "42318c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ara resources private limited', 'diraa hr services hiring for mncs', 'yogita staffing solution', 'techno endura', 'future solution centre', 'boyen haddin consulting and technol...', 'boyen haddin consulting and technol...', 'v-tech data outsourcing', 'ntt data information processing ser...', 'boyen haddin consulting and technol...']\n"
     ]
    }
   ],
   "source": [
    "print(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6449b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "38cd265a",
   "metadata": {},
   "source": [
    "Q2:Write a python program to scrape data for “Data Scientist” Job position in“Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter thelocation” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ba1ca1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d84568e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.shine.com/new/job-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "82926f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation & location as required into the question.\n",
    "\n",
    "job_title_input = driver.find_element(By.XPATH, \"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "\n",
    "job_title_input.send_keys('Data Scientist')\n",
    "\n",
    "location_input = driver.find_element(By.XPATH, \"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "\n",
    "location_input.send_keys('Banglore')\n",
    "\n",
    "#Now inspect search button find the class name & click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[2]/div/button\")\n",
    "\n",
    "search.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ce4a92c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sleep for some duration so all the data can loaded\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e354b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title=[]\n",
    "\n",
    "job_location=[]\n",
    "\n",
    "company_name=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f1c0ea6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Management', 'Data Management', 'Data Management']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#If we want to Scrap data from multiple pages then \n",
    "start = 0\n",
    "end = 3\n",
    "job_titles = []  # Initialize the list to store job titles\n",
    "\n",
    "for page in range(start, end):\n",
    "    title_elements = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard__jjUmu active white-box-border jobCard\"]/div/h2/a')\n",
    "    for i in title_elements:\n",
    "        job_titles.append(i.text)\n",
    "    \n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')  # To scrape data from the next page\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "print(job_titles)\n",
    "\n",
    "print(len(job_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1fb54dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management', 'Data Management']\n"
     ]
    }
   ],
   "source": [
    "#If we want to Scrap data from multiple pages then first 10 records for example\n",
    "start = 0\n",
    "end = 10  # Adjusted to scrape 10 records\n",
    "job_titles = []  # Initialize the list to store job titles\n",
    "\n",
    "for page in range(start, end):\n",
    "    title_elements = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard__jjUmu active white-box-border jobCard\"]/div/h2/a')\n",
    "    for i in title_elements:\n",
    "        job_titles.append(i.text)\n",
    "    \n",
    "    if page < end - 1:  # To avoid clicking on the next button on the last iteration\n",
    "        next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')  # To scrape data from the next page\n",
    "        next_button.click()\n",
    "        time.sleep(3)\n",
    "\n",
    "print(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "823175fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['China\\n+18', 'Bangalore', 'Gurugram\\n+1', 'Bangalore', 'Mumbai City\\n+1', 'Mumbai City\\n+4', 'Gurugram\\n+8', 'Canada\\n+14', 'Bangalore', 'Bangalore', 'Bangalore\\n+3', 'Gurugram\\n+6', 'Bangalore', 'Canada\\n+15', 'Other Odisha\\n+9', 'Gurugram\\n+9', 'Mumbai City\\n+6', 'Bangalore', 'Bangalore', 'Bangalore', 'Bangalore\\n+9', 'Bangalore', 'Bangalore', 'Gurugram\\n+9', 'Gurugram\\n+6', 'Gurugram\\n+6', 'Gurugram\\n+6', 'Gurugram\\n+9', 'Canada\\n+14', 'Gurugram\\n+6', 'Gurugram\\n+9', 'Bangalore\\n+9', 'Bangalore\\n+9', 'Gurugram\\n+6', 'Canada\\n+14', 'Canada\\n+15', 'Mumbai City\\n+4', 'Gurugram\\n+9', 'Australia\\n+17', 'Ethiopia\\n+18']\n"
     ]
    }
   ],
   "source": [
    "# Scraping Job Location from the given page\n",
    "location_tags = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for i in location_tags:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "    \n",
    "    \n",
    "print(job_location)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8ac1e3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['China\\n+18', 'Bangalore', 'Gurugram\\n+1', 'Bangalore', 'Mumbai City\\n+1', 'Mumbai City\\n+4', 'Gurugram\\n+8', 'Canada\\n+14', 'Bangalore', 'Bangalore']\n"
     ]
    }
   ],
   "source": [
    "#We want first 10 locations so above code become\n",
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10  # Number of records you want to scrape\n",
    "location_tags = []  # Initialize the list to store job titles\n",
    "scraped_records = 0  # Initialize counter for scraped records\n",
    "\n",
    "for page in range(start, end):\n",
    "    title_elements = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "    for i in title_elements:\n",
    "        location_tags.append(i.text)\n",
    "        scraped_records += 1\n",
    "        if scraped_records >= desired_records:\n",
    "            break  # Break the loop if desired records are scraped\n",
    "    \n",
    "    if scraped_records >= desired_records:\n",
    "        break  # Break the loop if desired records are scraped\n",
    "    \n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')  # To scrape data from the next page\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "print(location_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "72222099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Company name from the given page\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10\n",
    "company_names = []\n",
    "\n",
    "for page in range(start, end):\n",
    "    company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "    for i in company_tags:\n",
    "        company_name = i.text\n",
    "        company_names.append(company_name)\n",
    "        if len(company_names) >= desired_records:\n",
    "            break  # Break if desired number of records is reached\n",
    "    \n",
    "    if len(company_names) >= desired_records:\n",
    "        break  # Break if desired number of records is reached\n",
    "\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2abaf936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['future solution centre', 'infosys limited', 'ltimindtree limited', 'fondstaff private limited', 'digitalcube consultancy', 'techno endura', 'whiteslips global services private ...', 'yogita staffing solution', 'ara resources private limited', 'varite india private limited']\n"
     ]
    }
   ],
   "source": [
    "print(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af9a18f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>China\\n+18</td>\n",
       "      <td>future solution centre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>infosys limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Gurugram\\n+1</td>\n",
       "      <td>ltimindtree limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>fondstaff private limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Mumbai City\\n+1</td>\n",
       "      <td>digitalcube consultancy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Mumbai City\\n+4</td>\n",
       "      <td>techno endura</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Gurugram\\n+8</td>\n",
       "      <td>whiteslips global services private ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Canada\\n+14</td>\n",
       "      <td>yogita staffing solution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>ara resources private limited</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Management</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>varite india private limited</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Title         Location                                 Company\n",
       "0  Data Management       China\\n+18                  future solution centre\n",
       "1  Data Management        Bangalore                         infosys limited\n",
       "2  Data Management     Gurugram\\n+1                     ltimindtree limited\n",
       "3  Data Management        Bangalore               fondstaff private limited\n",
       "4  Data Management  Mumbai City\\n+1                 digitalcube consultancy\n",
       "5  Data Management  Mumbai City\\n+4                           techno endura\n",
       "6  Data Management     Gurugram\\n+8  whiteslips global services private ...\n",
       "7  Data Management      Canada\\n+14                yogita staffing solution\n",
       "8  Data Management        Bangalore           ara resources private limited\n",
       "9  Data Management        Bangalore            varite india private limited"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_titles, 'Location':location_tags,'Company':company_names})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ed471c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(location_tags),len(company_names))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0b67a39e",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the web page https://www.shine.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scrapeddata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c302943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.shine.com/new/job-search\")\n",
    "\n",
    "#Entering designation & location as required into the question.\n",
    "\n",
    "job_title_input = driver.find_element(By.XPATH, \"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "\n",
    "job_title_input.send_keys('Data Scientist')\n",
    "\n",
    "\n",
    "#Now inspect search button find the class name & then click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div/div/div[1]/div[1]/div/div[2]/div/div/form/div/div[2]/div/button\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9a8b82e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search button\n",
    "location_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/div[3]/div/div[1]/div/div[2]/div/ul/li[1]/button\")\n",
    "location_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e886704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#designation.send_keys('Delhi/NCR')\n",
    "select_location=driver.find_element(By.XPATH, \"/html/body/div[3]/div/div/div/div[3]/div/div/div/ul/li[13]/span/label\")\n",
    "select_location.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "ffd15eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#designation.send_keys('Delhi/NCR')\n",
    "show_button=driver.find_element(By.XPATH, \"/html/body/div[3]/div/div/div/div[4]/button[2]\")\n",
    "show_button.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5c77c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Click the search button\n",
    "salory_button = driver.find_element(By.XPATH, \"/html/body/div[1]/div[1]/div[3]/div/div/div/div[2]/div/ul/li[3]/button\")\n",
    "salory_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "27b538ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salory.send_keys('3-6')\n",
    "select_salory=driver.find_element(By.XPATH, \"/html/body/div[3]/div/div/div/div[3]/div/div/div/ul/li[3]/span/label\")\n",
    "select_salory.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "efb67ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#designation.send_keys('Delhi/NCR')\n",
    "show_filter_button=driver.find_element(By.XPATH, \"/html/body/div[3]/div/div/div/div[4]/button[2]\")\n",
    "show_filter_button.click()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bafae47",
   "metadata": {},
   "source": [
    "Creating the empty list to store the scraped data\n",
    "job_titles=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experiance_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bc047ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Data Scientist', 'Hiring candidates for Marketing with Fare & Data Analys ...', 'Hiring for Fare & Data Analyst-Travel process', 'Clinical Data Analyst', 'Clinical Data Management', 'Bioanalytical Research', 'Clinical SAS', 'Junior Clinical Data Management', 'Bioanalytical Research Associates', 'Data Engineer']\n"
     ]
    }
   ],
   "source": [
    "title_tags = driver.find_elements(By.XPATH, '//div[@class=\"parentClass position-relative\"]/div/div/div/h2/a')\n",
    "\n",
    "job_titles = []  # Initialize the list to store job titles\n",
    "count = 0\n",
    "\n",
    "for i in title_tags:\n",
    "    if count >= 10:\n",
    "        break\n",
    "    title = i.text\n",
    "    job_titles.append(title)\n",
    "    count += 1\n",
    "\n",
    "print(job_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "32650557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Delhi\\n+4', 'Delhi\\n+2', 'Delhi\\n+2', 'Delhi\\n+6', 'Delhi\\n+6', 'Delhi\\n+6', 'Delhi\\n+8', 'Delhi\\n+6', 'Delhi\\n+6', 'Delhi\\n+9']\n"
     ]
    }
   ],
   "source": [
    "#We want first 10 locations so above code become\n",
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10  # Number of records you want to scrape\n",
    "location_tags = []  # Initialize the list to store job titles\n",
    "scraped_records = 0  # Initialize counter for scraped records\n",
    "\n",
    "for page in range(start, end):\n",
    "    title_elements = driver.find_elements(By.XPATH, '//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "    for i in title_elements:\n",
    "        location_tags.append(i.text)\n",
    "        scraped_records += 1\n",
    "        if scraped_records >= desired_records:\n",
    "            break  # Break the loop if desired records are scraped\n",
    "    \n",
    "    if scraped_records >= desired_records:\n",
    "        break  # Break the loop if desired records are scraped\n",
    "    \n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')  # To scrape data from the next page\n",
    "    next_button.click()\n",
    "    time.sleep(3)\n",
    "\n",
    "print(location_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "05a687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Company name from the given page\n",
    "\n",
    "company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "desired_records = 10\n",
    "company_names = []\n",
    "\n",
    "for page in range(start, end):\n",
    "    company_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "    for i in company_tags:\n",
    "        company_name = i.text\n",
    "        company_names.append(company_name)\n",
    "        if len(company_names) >= desired_records:\n",
    "            break  # Break if desired number of records is reached\n",
    "    \n",
    "    if len(company_names) >= desired_records:\n",
    "        break  # Break if desired number of records is reached\n",
    "\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div[1]/div[1]/div[4]/div/div[2]/div[2]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "28014cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acme services private limited', 'sharda it services', 'sharda it services', 'techno endura', 'techno endura', 'techno endura', 'techno endura', 'techno endura', 'techno endura', 'future solution centre']\n"
     ]
    }
   ],
   "source": [
    "print(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c22e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Company Experience required\n",
    "experience_tags = driver.find_elements(By.XPATH, '//span[@class=\"ni-job-tuple-icon ni-job-tuple-icon-srp-experience exp\"]')\n",
    "for exp_tag in experience_tags:\n",
    "    exp = exp_tag.text\n",
    "    experience_required.append(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "84731874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 to 5 Yrs', 'Delhi\\n+4', '2 to 5 Yrs', 'Delhi\\n+2', '1 to 6 Yrs', 'Delhi\\n+2', '0 to 1 Yr', 'Delhi\\n+6', '0 to 1 Yr', 'Delhi\\n+6']\n"
     ]
    }
   ],
   "source": [
    "# Find elements containing experience required information\n",
    "experience_tags = driver.find_elements(By.XPATH, '//div[@class=\"jobCard_jobCard_lists__fdnsc\"]/div')\n",
    "\n",
    "experience_required = []  # Initialize the list to store experience required\n",
    "count = 0\n",
    "\n",
    "for exp_tag in experience_tags:\n",
    "    if count >= 10:\n",
    "        break\n",
    "    exp = exp_tag.text\n",
    "    experience_required.append(exp)\n",
    "    count += 1\n",
    "\n",
    "print(experience_required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "48ecadec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company</th>\n",
       "      <th>Experiance Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi\\n+4</td>\n",
       "      <td>acme services private limited</td>\n",
       "      <td>3 to 5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hiring candidates for Marketing with Fare &amp; Da...</td>\n",
       "      <td>Delhi\\n+2</td>\n",
       "      <td>sharda it services</td>\n",
       "      <td>Delhi\\n+4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hiring for Fare &amp; Data Analyst-Travel process</td>\n",
       "      <td>Delhi\\n+2</td>\n",
       "      <td>sharda it services</td>\n",
       "      <td>2 to 5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>Delhi\\n+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical Data Management</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>1 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bioanalytical Research</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>Delhi\\n+2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Clinical SAS</td>\n",
       "      <td>Delhi\\n+8</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Junior Clinical Data Management</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bioanalytical Research Associates</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>Delhi\\n+9</td>\n",
       "      <td>future solution centre</td>\n",
       "      <td>Delhi\\n+6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Location  \\\n",
       "0                                     Data Scientist  Delhi\\n+4   \n",
       "1  Hiring candidates for Marketing with Fare & Da...  Delhi\\n+2   \n",
       "2      Hiring for Fare & Data Analyst-Travel process  Delhi\\n+2   \n",
       "3                              Clinical Data Analyst  Delhi\\n+6   \n",
       "4                           Clinical Data Management  Delhi\\n+6   \n",
       "5                             Bioanalytical Research  Delhi\\n+6   \n",
       "6                                       Clinical SAS  Delhi\\n+8   \n",
       "7                    Junior Clinical Data Management  Delhi\\n+6   \n",
       "8                  Bioanalytical Research Associates  Delhi\\n+6   \n",
       "9                                      Data Engineer  Delhi\\n+9   \n",
       "\n",
       "                         Company Experiance Required  \n",
       "0  acme services private limited          3 to 5 Yrs  \n",
       "1             sharda it services           Delhi\\n+4  \n",
       "2             sharda it services          2 to 5 Yrs  \n",
       "3                  techno endura           Delhi\\n+2  \n",
       "4                  techno endura          1 to 6 Yrs  \n",
       "5                  techno endura           Delhi\\n+2  \n",
       "6                  techno endura           0 to 1 Yr  \n",
       "7                  techno endura           Delhi\\n+6  \n",
       "8                  techno endura           0 to 1 Yr  \n",
       "9         future solution centre           Delhi\\n+6  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title':job_titles, 'Location':location_tags,'Company':company_names,'Experiance Required':experience_required})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "380c2509",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "6. Brand\n",
    "7. ProductDescription\n",
    "8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search fieldwhere “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "97a7bd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "f8fa10ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation & location as required into the question.\n",
    "\n",
    "search_field_input = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input\")\n",
    "\n",
    "search_field_input .send_keys('sunglasses')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8352f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field_input = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button\")\n",
    "\n",
    "search_field_input.click()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd0483fb",
   "metadata": {},
   "source": [
    "#Next button\n",
    "/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\n",
    "\n",
    "\n",
    "#prise\n",
    "//div[@class=\"_30jeq3\"]\n",
    "#Discription\n",
    "//a[@class=\"IRpwTa\"]\n",
    "#brand Namwe\n",
    "//div[@class=\"_2WkVRV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbcc85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store scraped data\n",
    "    brands = []\n",
    "    descriptions = []\n",
    "    prices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping Company Experience required\n",
    "product_list=[]\n",
    "product_tags = driver.find_elements(By.XPATH, '//div[@class=\"_2WkVRV\"]')\n",
    "for prod_tag in product_tags:\n",
    "    prod = prod_tag.text\n",
    "    product_list.append(prod)\n",
    "\n",
    "    \n",
    "print(product_list)    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "bf950c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elligator', 'Elligator', 'PIRASO', 'Fastrack', 'Fastrack', 'VINCENT CHASE', 'Elligator', 'Elligator', 'Fastrack', 'DEIXELS', 'Elligator', 'Elligator', 'PIRASO', 'Elligator', 'Rich Club', 'PIRASO', 'Elligator', 'Eyenaks', 'PIRASO', 'ROYAL SON', 'Lenskart STUDIO', 'ROYAL SON', 'VINCENT CHASE', 'Fastrack', 'ROADWAY', 'RESIST EYEWEAR', 'Eyewearlabs', 'ROZZETTA CRAFT', 'Rich Club', 'ROYAL SON', 'PROVOGUE', 'Lenskart STUDIO', 'VINCENT CHASE', 'PIRASO', 'VINCENT CHASE', 'VINCENT CHASE', 'Eyewearlabs', 'ROZZETTA CRAFT', 'iCopertina', 'VINCENT CHASE', 'Elligator', 'Elligator', 'VINCENT CHASE', 'iCopertina', 'VINCENT CHASE', 'PIRASO', 'Elligator', 'Elligator', 'VINCENT CHASE', 'Singco India', 'Elligator', 'Eyenaks', 'ROZZETTA CRAFT', 'Fastrack', 'Singco India', 'VINCENT CHASE', 'Lenskart STUDIO', 'ROYAL SON', 'VINCENT CHASE', 'Mast & Harbour', 'ROZZETTA CRAFT', 'PROVOGUE', 'PROVOGUE', 'VINCENT CHASE', 'VINCENT CHASE', 'ROZZETTA CRAFT', 'Eyewearlabs', 'Lenskart STUDIO', 'VINCENT CHASE', 'VINCENT CHASE', 'ROYAL SON', 'Elligator', 'Mast & Harbour', 'ROZZETTA CRAFT', 'Ray-Ban', 'VINCENT CHASE', 'ROYAL SON', 'Elligator', 'PIRASO', 'VINCENT CHASE', 'Eyewearlabs', 'Hooper', 'VINCENT CHASE', 'ROZZETTA CRAFT', 'VINCENT CHASE', 'VINCENT CHASE', 'Hooper', 'AISLIN', 'Roadster', 'Elligator', 'ROYAL SON', 'ROYAL SON', 'Fastrack', 'VINCENT CHASE', 'Mast & Harbour', 'PIRASO', 'RESIST EYEWEAR', 'ROYAL SON', 'ROADWAY', 'hipe', 'ROYAL SON', 'ROYAL SON', 'ROYAL SON', 'OAKLEY', 'PC STAR', 'ROYAL SON', 'ROYAL SON', 'AISLIN', 'Ray-Ban', 'hipe', 'ROYAL SON', 'ROZZETTA CRAFT', 'PIRASO', 'NuVew', 'ROYAL SON', 'Fastrack', 'ROYAL SON', 'Lenskart STUDIO', 'ROYAL SON', 'Singco India']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "product_list = []\n",
    "\n",
    "# Define the range of pages to scrape\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(start, end):\n",
    "    # Find all product elements on the page\n",
    "    product_tags = driver.find_elements(By.XPATH, '//div[@class=\"_2WkVRV\"]')\n",
    "    \n",
    "    # Extract product information from each element\n",
    "    for prod_tag in product_tags:\n",
    "        prod = prod_tag.text\n",
    "        product_list.append(prod)\n",
    "    \n",
    "    # Click on the next page button\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the list of products scraped from all pages\n",
    "print(product_list)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e7db12f",
   "metadata": {},
   "source": [
    "brand_elements = driver.find_elements(By.XPATH, \"//div[@class='_2WkVRV']\")\n",
    "brands = []\n",
    "# Scrape brand from each listing\n",
    "for brand_element in brand_elements:\n",
    "    brand = brand_element.text\n",
    "    brands.append(brand)\n",
    "    \n",
    "print(brands)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d11abb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hooper', 'Lenskart STUDIO', 'ROZZETTA CRAFT', 'PROVOGUE', 'PIRASO', 'Eyenaks', 'ROYAL SON', 'ROYAL SON', 'Zeemork', 'Fastrack', 'ROYAL SON', 'NuVew', 'ROADWAY', 'VINCENT CHASE', 'Aarna Fashion', 'VINCENT CHASE', 'ROYAL SON', 'Royaltail', 'HRX by Hrithik Roshan', 'Arzonai', 'AISLIN', 'ROYAL SON', 'HRX by Hrithik Roshan', 'ROZZETTA CRAFT', 'VINCENT CHASE', 'VINCENT CHASE', 'KAEN EYEWEAR', 'ROYAL SON', 'VINCENT CHASE', 'peter india', 'ROYAL SON', 'AISLIN', 'Ray-Ban', 'VINCENT CHASE', 'PIRASO', 'Aarna Fashion', 'ROYAL SON', 'ROYAL SON', 'VINCENT CHASE', 'VINCENT CHASE', 'Eyenaks', 'Lenskart STUDIO', 'AISLIN', 'Dressberry', 'VINCENT CHASE', 'Zeemork', 'ROYAL SON', 'ROZZETTA CRAFT', 'PIRASO', 'GUESS', 'Eyewearlabs', 'PROVOGUE', 'PIRASO', 'Aarna Fashion', 'VINCENT CHASE', 'VINCENT CHASE', 'Eyewearlabs', 'Lenskart STUDIO', 'Arzonai', 'PIRASO', 'ROYAL SON', 'ROZZETTA CRAFT', 'ROYAL SON', 'GANSTA', 'VINCENT CHASE', 'VINCENT CHASE', 'ROYAL SON', 'ROZZETTA CRAFT', 'peter india', 'VINCENT CHASE', 'Irayz', 'ROYAL SON', 'VINCENT CHASE', 'ROYAL SON', 'Aarna Fashion', 'PIRASO', 'ROZZETTA CRAFT', 'Eyewearlabs', 'PIRASO', 'Singco India', 'Ann Taylor', 'ROZZETTA CRAFT', 'Singco India', 'NuVew', 'iCopertina', 'ROYAL SON', 'ROYAL SON', 'PROVOGUE', 'VINCENT CHASE', 'Elligator', 'Eyewearlabs', 'ROYAL SON', 'Fastrack', 'AISLIN', 'VINCENT CHASE', 'VINCENT CHASE', 'ROYAL SON', 'AISLIN', 'Singco India', 'ROYAL SON']\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "brands = []\n",
    "record_count = 0\n",
    "\n",
    "while record_count < 100:  # Scrape until 100 records are obtained\n",
    "    # Find all brand elements on the page\n",
    "    brand_elements = driver.find_elements(By.XPATH, \"//div[@class='_2WkVRV']\")\n",
    "    \n",
    "    # Extract brand from each listing\n",
    "    for brand_element in brand_elements:\n",
    "        brand = brand_element.text\n",
    "        brands.append(brand)\n",
    "        record_count += 1  # Increment the record count\n",
    "        \n",
    "        if record_count == 100:  # Break the loop if 100 records are obtained\n",
    "            break\n",
    "    \n",
    "    # Click on the next page button\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the list of brands scraped\n",
    "print(brands)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c16fd283",
   "metadata": {},
   "source": [
    "# Function to scrape product description from the current page\n",
    "descriptions = []\n",
    " \n",
    "# Find all the product descriptions\n",
    "description_elements = driver.find_elements(By.XPATH, \"//a[@class='IRpwTa']\")\n",
    "    \n",
    "# Scrape product description from each listing\n",
    "for desc_element in description_elements:\n",
    "    description = desc_element.text\n",
    "    descriptions.append(description)\n",
    "\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "230c90ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Toughened Glass Lens, UV Protection, Riding Glasses Way...', 'UV Protection Sports Sunglasses (69)', 'UV Protection Cat-eye, Retro Square, Oval, Round Sungla...', 'UV Protection, Night Vision Aviator Sunglasses (58)', 'Gradient Round Sunglasses (Free Size)', 'UV Protection, Mirrored Aviator Sunglasses (58)', 'UV Protection, Gradient Round Sunglasses (58)', 'by Lenskart Polarized, UV Protection Aviator Sunglasses...', 'UV Protection Wayfarer Sunglasses (56)', 'Polarized, UV Protection Retro Square Sunglasses (56)', 'UV Protection, Gradient Round Sunglasses (58)', 'Night Vision, Gradient, UV Protection Wrap-around Sungl...', 'Gradient Aviator Sunglasses (58)', 'UV Protection, Gradient Rectangular Sunglasses (Free Si...', 'UV Protection, Gradient Retro Square Sunglasses (Free S...', 'Polarized Rectangular Sunglasses (56)', 'Polarized, UV Protection Wayfarer, Retro Square Sunglas...', 'UV Protection Wayfarer Sunglasses (56)', 'Toughened Glass Lens, UV Protection Wayfarer, Rectangul...', 'UV Protection, Gradient Butterfly, Retro Square Sunglas...', 'UV Protection, Gradient Cat-eye Sunglasses (61)', 'UV Protection Wayfarer Sunglasses (Free Size)', 'UV Protection, Mirrored Sports Sunglasses (62)', 'Rectangular Sunglass', 'UV Protection, Gradient Round, Wayfarer Sunglasses (60)', 'Polarized Aviator Sunglasses (56)', 'UV Protection Cat-eye Sunglasses (Free Size)', 'by Lenskart Polarized, UV Protection Wayfarer Sunglasse...', 'Polarized, UV Protection Sports, Wrap-around Sunglasses...', 'Polarized, UV Protection Aviator Sunglasses (58)', 'UV Protection Retro Square Sunglasses (58)', 'by Lenskart Polarized, UV Protection Round Sunglasses (...', 'UV Protection Retro Square Sunglasses (54)', 'UV Protection Shield Sunglasses (54)', 'UV Protection, Gradient Over-sized Sunglasses (59)', 'UV Protection Cat-eye Sunglasses (51)', 'by Lenskart UV Protection Cat-eye Sunglasses (60)', 'Mirrored Rectangular Sunglasses (59)', 'UV Protection Rectangular Sunglasses (Free Size)', 'UV Protection Rectangular Sunglasses (52)', 'by Lenskart UV Protection Cat-eye Sunglasses (59)', 'by Lenskart Polarized, UV Protection Clubmaster Sunglas...', 'UV Protection Wrap-around Sunglasses (Free Size)', 'UV Protection, Gradient Cat-eye Sunglasses (58)', 'Mirrored Aviator Sunglasses (58)', 'UV Protection, Gradient Rectangular Sunglasses (Free Si...', 'by Lenskart Polarized, UV Protection Round Sunglasses (...', 'by Lenskart Polarized, UV Protection Aviator Sunglasses...', 'Polarized, UV Protection Wayfarer Sunglasses (51)', 'UV Protection Round Sunglasses (Free Size)', 'UV Protection Cat-eye Sunglasses (56)', 'UV Protection Retro Square Sunglasses (58)', 'UV Protection Sports, Wrap-around Sunglasses (70)', 'UV Protection Round Sunglasses (Free Size)', 'Polarized, UV Protection Rectangular Sunglasses (65)', 'Polarized, UV Protection Rectangular Sunglasses (54)', 'UV Protection Aviator Sunglasses (Free Size)', 'UV Protection Round Sunglasses (54)', 'Polarized, UV Protection Wayfarer, Retro Square Sunglas...', 'UV Protection Aviator Sunglasses (62)', 'UV Protection, Gradient, Riding Glasses Over-sized Sung...', 'by Lenskart Polarized, UV Protection Round Sunglasses (...', 'UV Protection Cat-eye, Retro Square, Oval, Round Sungla...', 'by Lenskart Polarized, UV Protection Round Sunglasses (...', 'Gradient Retro Square Sunglasses (60)', 'UV Protection Spectacle Sunglasses (Free Size)', 'UV Protection Over-sized Sunglasses (64)', 'UV Protection Sports Sunglasses (55)', 'UV Protection, Riding Glasses Retro Square Sunglasses (...', 'UV Protection Round Sunglasses (50)', 'UV Protection, Gradient Cat-eye Sunglasses (51)', 'UV Protection Aviator Sunglasses (58)', 'by Lenskart Polarized, UV Protection Wayfarer Sunglasse...', 'UV Protection Rectangular Sunglasses (Free Size)', 'Polarized, UV Protection Aviator Sunglasses (58)', 'UV Protection Spectacle Sunglasses (Free Size)', 'UV Protection Oval Sunglasses (60)', 'UV Protection, Gradient Oval Sunglasses (60)', 'UV Protection Retro Square Sunglasses (60)', 'Photochromatic Lens, UV Protection Aviator Sunglasses (...', 'by Lenskart Polarized, UV Protection Round Sunglasses (...', 'by Lenskart Polarized, UV Protection Cat-eye Sunglasses...', 'by Lenskart Polarized, UV Protection Wayfarer Sunglasse...', 'UV Protection Wayfarer, Rectangular Sunglasses (61)', 'UV Protection Cat-eye, Butterfly Sunglasses (56)', 'by Lenskart Polarized, UV Protection Aviator Sunglasses...', 'UV Protection Sports Sunglasses (Free Size)', 'UV Protection, Gradient Butterfly, Wayfarer Sunglasses ...', 'Polarized, UV Protection Retro Square Sunglasses (62)', 'UV Protection Round Sunglasses (53)', 'Gradient Retro Square Sunglasses (60)', 'UV Protection Cat-eye Sunglasses (55)', 'UV Protection, Gradient Over-sized Sunglasses (63)', 'UV Protection Rectangular Sunglasses (51)', 'Gradient Aviator Sunglasses (60)', 'UV Protection, Gradient Over-sized Sunglasses (62)', 'Polarized, UV Protection Aviator Sunglasses (61)', 'UV Protection, Gradient Butterfly, Retro Square Sunglas...', 'Polarized, UV Protection Rectangular Sunglasses (58)', 'UV Protection Over-sized Sunglasses (60)']\n"
     ]
    }
   ],
   "source": [
    "descriptions = []\n",
    "\n",
    "# Define a counter to keep track of the number of records scraped\n",
    "record_count = 0\n",
    "\n",
    "# Iterate over each page\n",
    "while record_count < 100:\n",
    "    # Find all the product descriptions on the current page\n",
    "    description_elements = driver.find_elements(By.XPATH, \"//a[@class='IRpwTa']\")\n",
    "    \n",
    "    # Scrape product description from each listing on the current page\n",
    "    for desc_element in description_elements:\n",
    "        description = desc_element.text\n",
    "        descriptions.append(description)\n",
    "        record_count += 1\n",
    "        \n",
    "        # Check if the desired number of records (100) has been reached\n",
    "        if record_count >= 100:\n",
    "            break  # Exit the loop if 100 records have been scraped\n",
    "    \n",
    "    # Check if 100 records have been scraped before clicking on the next page\n",
    "    if record_count >= 100:\n",
    "        break  # Exit the loop if 100 records have been scraped\n",
    "    \n",
    "    # Click on the next page button\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[12]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the list of descriptions containing 100 records\n",
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c2de443d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['₹418', '₹678', '₹739', '₹902', '₹899', '₹599', '₹645', '₹725', '₹899', '₹199', '₹475', '₹854', '₹1,899', '₹2,189', '₹244', '₹549', '₹445', '₹854', '₹1,449', '₹1,619', '₹559', '₹725', '₹426', '₹1,199', '₹279', '₹1,999', '₹1,499', '₹673', '₹899', '₹418', '₹1,199', '₹559', '₹863', '₹2,916', '₹9,368', '₹599', '₹681', '₹679', '₹779', '₹589']\n"
     ]
    }
   ],
   "source": [
    "# Function to scrape price from the current page\n",
    "price_elements = driver.find_elements(By.XPATH, \"//div[@class='_30jeq3']\")\n",
    "\n",
    "prices = []\n",
    "\n",
    "# Scrape price from each listing\n",
    "for pri_element in price_elements:\n",
    "    price = pri_element.text\n",
    "    prices.append(price)  \n",
    "\n",
    "print(prices)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "e3f2b1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['₹346', '₹999', '₹169', '₹129', '₹189', '₹169', '₹199', '₹359', '₹159', '₹443', '₹385', '₹1,236', '₹159', '₹399', '₹499', '₹539', '₹634', '₹2,199', '₹399', '₹549', '₹1,899', '₹594', '₹569', '₹619', '₹629', '₹749', '₹597', '₹672', '₹179', '₹169', '₹399', '₹374', '₹388', '₹539', '₹224', '₹599', '₹404', '₹636', '₹149', '₹299', '₹374', '₹404', '₹556', '₹210', '₹819', '₹919', '₹636', '₹379', '₹539', '₹395', '₹1,999', '₹399', '₹199', '₹169', '₹187', '₹232', '₹711', '₹1,199', '₹279', '₹599', '₹484', '₹299', '₹349', '₹569', '₹1,199', '₹399', '₹594', '₹551', '₹594', '₹644', '₹359', '₹1,999', '₹1,236', '₹849', '₹299', '₹699', '₹599', '₹404', '₹811', '₹299', '₹559', '₹681', '₹759', '₹105', '₹429', '₹699', '₹679', '₹974', '₹629', '₹2,499', '₹376', '₹625', '₹1,479', '₹799', '₹539', '₹569', '₹636', '₹375', '₹1,799', '₹129']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the list to store prices\n",
    "prices = []\n",
    "\n",
    "# Define the range of pages to scrape\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "# Counter to keep track of scraped records\n",
    "scraped_records = 0\n",
    "\n",
    "# Iterate over each page\n",
    "for page in range(start, end):\n",
    "    # Find all price elements on the page\n",
    "    price_elements = driver.find_elements(By.XPATH, \"//div[@class='_30jeq3']\")\n",
    "    \n",
    "    # Scrape price from each listing on the current page\n",
    "    for pri_element in price_elements:\n",
    "        price = pri_element.text\n",
    "        prices.append(price)\n",
    "        scraped_records += 1\n",
    "        \n",
    "        # Check if we have scraped enough records\n",
    "        if scraped_records >= 100:\n",
    "            break\n",
    "    \n",
    "    # Check if we have scraped enough records\n",
    "    if scraped_records >= 100:\n",
    "        break\n",
    "    \n",
    "    # Click on the next page button\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Wait for the page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the list of prices of the first 100 records\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "3e5ce069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brands</th>\n",
       "      <th>Discription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hooper</td>\n",
       "      <td>Toughened Glass Lens, UV Protection, Riding Gl...</td>\n",
       "      <td>₹346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lenskart STUDIO</td>\n",
       "      <td>UV Protection Sports Sunglasses (69)</td>\n",
       "      <td>₹999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Cat-eye, Retro Square, Oval, Rou...</td>\n",
       "      <td>₹169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PROVOGUE</td>\n",
       "      <td>UV Protection, Night Vision Aviator Sunglasses...</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>Gradient Round Sunglasses (Free Size)</td>\n",
       "      <td>₹189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>UV Protection, Gradient Over-sized Sunglasses ...</td>\n",
       "      <td>₹569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized, UV Protection Aviator Sunglasses (61)</td>\n",
       "      <td>₹636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Retro Squar...</td>\n",
       "      <td>₹375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Polarized, UV Protection Rectangular Sunglasse...</td>\n",
       "      <td>₹1,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>UV Protection Over-sized Sunglasses (60)</td>\n",
       "      <td>₹129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Brands                                        Discription   Price\n",
       "0            Hooper  Toughened Glass Lens, UV Protection, Riding Gl...    ₹346\n",
       "1   Lenskart STUDIO               UV Protection Sports Sunglasses (69)    ₹999\n",
       "2    ROZZETTA CRAFT  UV Protection Cat-eye, Retro Square, Oval, Rou...    ₹169\n",
       "3          PROVOGUE  UV Protection, Night Vision Aviator Sunglasses...    ₹129\n",
       "4            PIRASO              Gradient Round Sunglasses (Free Size)    ₹189\n",
       "..              ...                                                ...     ...\n",
       "95    VINCENT CHASE  UV Protection, Gradient Over-sized Sunglasses ...    ₹569\n",
       "96        ROYAL SON   Polarized, UV Protection Aviator Sunglasses (61)    ₹636\n",
       "97           AISLIN  UV Protection, Gradient Butterfly, Retro Squar...    ₹375\n",
       "98     Singco India  Polarized, UV Protection Rectangular Sunglasse...  ₹1,799\n",
       "99        ROYAL SON           UV Protection Over-sized Sunglasses (60)    ₹129\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Brands':brands, 'Discription':descriptions,'Price':prices})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e097286",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/productreviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\n",
    "place=FLIPKART\n",
    "\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "f1a7dc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dddc43e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '5', '5', '5', '5', '5', '5', '5', '5', '5']\n"
     ]
    }
   ],
   "source": [
    "rating_tags = driver.find_elements(By.XPATH, '//div[@class=\"col _2wzgFH K0kLPL\"]/div[1]/div')  # Adjusted XPath to target only the rating element\n",
    "ratings = []\n",
    "\n",
    "for tag in rating_tags:\n",
    "    rating = tag.text.split()[0]  # Splitting the text and extracting the rating\n",
    "    ratings.append(rating)\n",
    "\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c27a2cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5', '5', '5', '5', '5', '5', '4', '5', '5', '5', '4', '5', '5', '5', '5', '5', '5', '5', '4', '1', '5', '5', '5', '5', '5', '5', '5', '5', '3', '5', '1', '4', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '4', '5', '5', '4', '5', '4', '5', '5', '5', '5', '5', '3', '5', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '1', '5', '5', '1', '5', '5', '5', '1', '4', '5', '5', '5', '5', '4', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '5', '2', '4']\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store ratings\n",
    "ratings = []\n",
    "\n",
    "# Set the start and end pages\n",
    "start_page = 0\n",
    "end_page = 11  # Adjusted to scrape 100 records\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(start_page, end_page):\n",
    "    # Find all rating elements on the current page\n",
    "    rating_tags = driver.find_elements(By.XPATH, '//div[@class=\"col _2wzgFH K0kLPL\"]/div[1]/div')\n",
    "\n",
    "    # Scraping ratings from the current page\n",
    "    for tag in rating_tags:\n",
    "        rating = tag.text.split()[0]  # Splitting the text and extracting the rating\n",
    "        ratings.append(rating)\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if len(ratings) >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped ratings\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c4d46dd",
   "metadata": {},
   "source": [
    "review_summary_tags = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')  # Adjusted XPath to target only the rating element\n",
    "review_summary = []\n",
    "\n",
    "for tag in review_summary_tags:\n",
    "    rating = tag.text.split()[0]\n",
    "    review_summary.append(rating)\n",
    "\n",
    "print(review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4a716e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Super!', 'Absolutely Awesome device', 'Really Awesome', 'Simply awesome', 'Perfect product!', 'Terrific', 'Classy product', 'Excellent', 'Slightly disappointed', 'Value-for-money', 'Excellent', 'Super!', 'Worth every penny', 'Mind-blowing purchase', 'Worth every penny', 'Pretty good', 'Useless product', 'Super!', 'Super!', 'Mind-blowing purchase', 'Perfect product!', 'Terrific', 'Just wow!', 'Great product', 'Great product', 'Value-for-money', 'Worth every penny', 'Good choice', 'Highly recommended', 'Moderate', 'Worthless', 'Worth every penny', 'Super!', 'Highly recommended', 'Classy product', 'Excellent', 'Nice', 'Highly recommended', 'Classy product', 'Awesome', 'Very Good', 'Best in the market!', 'Wonderful', 'Wonderful', 'Brilliant', 'Worth every penny', 'genuine Product.', 'Very Good', 'Value-for-money', 'Worth the money', 'Just wow!', 'Excellent', 'Fabulous!', 'Pretty good', 'Utterly Disappointed', 'Excellent', 'Perfect product!', 'Brilliant', 'Best in the market!', 'Excellent', 'Simply awesome', 'Highly recommended', 'Good quality product', 'Awesome', 'Great product', 'Terrific purchase', 'Just wow!', 'Worth every penny', 'Best in the market!', 'Excellent', 'Super!', 'Awesome', 'Really Nice', 'Super!', 'Just okay', 'Just wow!', 'Brilliant', 'Simply awesome', 'Nice product', 'Brilliant', 'Mind-blowing purchase', 'Perfect product!', 'Great product', 'Fabulous!', 'Great product', 'Perfect product!', 'Good choice', 'Brilliant', 'Highly recommended', 'Good', 'Terrific purchase', 'Great product', 'Delightful', 'Classy product', 'Best in the market!', 'Highly recommended', 'Fabulous!', 'Very Good', 'Mind-blowing purchase', 'Highly recommended']\n"
     ]
    }
   ],
   "source": [
    "review_summaries = []\n",
    "\n",
    "# Define the range for scraping pages\n",
    "start = 0\n",
    "end = 100  # Set the desired number of records to scrape\n",
    "\n",
    "for page in range(start, end):\n",
    "    # Scraping review summaries from the current page\n",
    "    review_summary_tags = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "    for tag in review_summary_tags:\n",
    "        summary = tag.text\n",
    "        review_summaries.append(summary)\n",
    "        if len(review_summaries) >= 100:  # Exit loop if 100 records are scraped\n",
    "            break\n",
    "    \n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Add a short delay to allow the next page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    if len(review_summaries) >= 100:  # Exit loop if 100 records are scraped\n",
    "        break\n",
    "\n",
    "# Display the scraped review summaries\n",
    "print(review_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "9203980d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everything', 'hello', 'Best', 'Great', 'The', 'Looks', 'Flipkart', 'best', 'Great', 'It']\n"
     ]
    }
   ],
   "source": [
    "#Full review\n",
    "\n",
    "fullreview_summary_tags = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]/div/div')  # Adjusted XPath to target only the rating element\n",
    "fullreview_summary = []\n",
    "\n",
    "for tag in fullreview_summary_tags:\n",
    "    review = tag.text.split()[0]\n",
    "    fullreview_summary.append(review)\n",
    "\n",
    "print(fullreview_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "7bf07898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For 30k nice', 'Superb!!!!!', 'Nice', 'Very satisfy product', 'Am facing heating issues', 'Bought it for 36k in bbd\\nWorth for it\\nSometimes it heats below the camera\\nexpect that everything is fine', 'Osm camera 📷 quality and battery backup is best I love ❤️ apple products thanks you Flipkart 😍', 'Camera of iPhone 11 is just amazing . Battery backup is little bit less require 2 times charging for 7 hrs screen time. Display is awesome..\\nNetwork issue is there .. Connectivity is very poor', 'Camera and gaming is to good... Only 1% issue battery backup is not good because Iam gamer and I charge 3 to 4 time in a day', 'Nice 👍', 'Good', 'Well this my first experience with an iPhone and it turned out to be great. Awesome product, brilliant camera, amazing performance. Using it in 2022 and still not out of date till 2024 at least. Just one thing that 5g is not supported. If you can stick to 4g then go for it. Best budget iPhone ever.', 'Sim slot is not working properly', 'I phone is more than enough 😜\\nBest camera\\nBest display\\nBest performance\\nBut no charger inside box why 🤨', 'You can buy this product very good performance I love this product ❤️❤️', 'I am very happy', 'I love it first I phone', 'Good choice . I have 12 pro also but i feel this is kore worth . Good performance and budget friendly', 'Design is good. Battery draining too fast. camera average. Network frequency is very bad', 'Good', 'Volume button not working', 'Nice ! Mobile Phone Nice design buy it 👍', 'Amazing Experience', 'I love this phone 🥰😍', 'Bed', 'Very good phone Thank you Flipkart', 'Best sarwis boy man best', 'good one', 'The delivery was seamless and the executive was very friendly. The package was good as well', 'My first iPhone and superb quality of camera and long lasting battery superb mobile', 'Best phone in the world', 'Superb', 'The battery conked off in a matter of a few hours. It was delivered to me on 25.09.2022 and on the very same day after a few hours the phone has stopped working. Now the phone neither would charge nor would it power on. I specifically had used the original accessories while plugging in the device. I will have to think twice before purchasing or buying any electronic device from online platform in the future.', 'Binod iphone lover', 'Nice I love it 😍😍😍❤❤❤\\nMy girlfriend so happy', 'Bad collety', 'Superb performance 🔥🔥', 'good', 'Very good 👌', 'Nice 👍', 'Very good', 'Good', 'Superb', 'Good mobiles', 'Thx', 'Best of All Time🔥', 'Nyc phone 😃😃', 'Excellent good', 'Nice mobile', 'This phone is very versatile and camera is very good especially in low light. Also design is very stylish and comfortable in hand.', 'Really awesome product .\\nPerformance is great .\\nBattery life is good\\nCamera quality just fantastic .', 'Nice product', 'Good 👍', 'Good', 'osm product.....i loved it', 'Overgood excellent condition I phone 11 📱 🥰❤', 'Very good product and very slim. But the only thing is there is no charger in the box.', 'Good phone..budget friendly', 'just love it❤️❤️❤️❤️', 'Excellent Product.', 'Thank you Flipkart for delivering my ph in the given time period and in perfect condition soo far ....keep shine', 'Good product', 'Excellent', 'Best one', 'Awesome', 'Nice product', 'I am using the iPhone from the 4s and so then I continued up to 8 plus and then I switched back to android as the price iPhone it is not give anything new. But I get to hear about the new features of IOS 16 so I started back from the 11 models again . and I am happy about it but IOS 16 waiting for the update eagerly', 'Battery backup is so bad.', 'So nice', 'Awesome phone but battery backup is not good', 'Open box delivery:- No worries of receiving fake items or a soap bar.\\nThe product is genuine.\\nPrice is reasonable.\\nWho needs more of an explanation for an iPhone. Totally worth it.', 'So Happy', 'Nice product thank you Flipkart ❤️', 'Excellent phone, thanks flipkart my dream phone is send to me', 'Wonderful product.best phone', 'Prolly the best phone I have used so far', 'Thanks Flipkart', 'First time I order from Flipkart when I ordered I have so Many daught after receiving my order till now I m satisfy', 'Very good product, battery life is decent', 'It was good looking & great working\\nI loved it.🍎', 'Good', \"upgraded iPhone 8 to iPhone 11, Phone working very smooth and faster performance, i didn't checked camera quality, ill update later.\", 'One of the best in this price range go for it without any doubt. everything is good in this phone except battery life, it could be more my 2 yrs old one plus 8 battery life is same as i phone 11 but it is sufficient for the whole day if you are not a heavy user ..', 'No No doubt the product is good but the additional cost like Gaana subscription and BYJU’S classes with all the people who does not need this have to pay for this this going through but it is not actually feel they can drop the price remaining by those subscription', 'Superb', 'Value for money', \"Love this phone as it is my First IOS device.\\nOnly Con is that it heats very soon as you start to use other than that it's worth buying\", 'Wow super iPhone 11', 'Love', 'Very nice mobile', 'Really good product.', 'Bought this for my sister. She was checking and round phones for 30K. I suggested exchanging an old phone and try this. Unlike other phones, these usually have software updates for 3 to 5 years. Also best in budget and more than enough even for a heavy user like me. Battery is decent not the best .', 'Very good product', 'This is osm and battery backup all good ❤️', 'Superb unbelievable my wife surprised gift she is full happy thank u flipkart', 'Superb', 'Very nice phone.. thanks to flipcart', 'Good mobile with great things. But I felt battery performance alone somehow it’s not good. While using camera that will suck your battery lot.', 'Awesome Performance 🔥🔥🔥🔥🔥', 'best camera\\nbest os\\nlightning fast speed\\nloved the 🍎 11\\nnight mode killing']\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store scraped data\n",
    "full_reviews = []\n",
    "\n",
    "# Define the range for scraping pages\n",
    "start = 0\n",
    "end = 100  # Set the desired number of records to scrape\n",
    "\n",
    "for page in range(start, end):\n",
    "    # Scraping full reviews from the current page\n",
    "    full_review_tags = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]/div/div')\n",
    "    for tag in full_review_tags:\n",
    "        review = tag.text\n",
    "        full_reviews.append(review)\n",
    "        if len(full_reviews) >= 100:  # Exit loop if 100 records are scraped\n",
    "            break\n",
    "    \n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div/div/div[2]/div[13]/div/div/nav/a[12]')\n",
    "    next_button.click()\n",
    "    \n",
    "    # Add a short delay to allow the next page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    if len(full_reviews) >= 100:  # Exit loop if 100 records are scraped\n",
    "        break\n",
    "\n",
    "# Display the scraped full reviews\n",
    "print(full_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "1406df3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Super!</td>\n",
       "      <td>For 30k nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely Awesome device</td>\n",
       "      <td>Superb!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Really Awesome</td>\n",
       "      <td>Nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Very satisfy product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Am facing heating issues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>Superb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Very nice phone.. thanks to flipcart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Good mobile with great things. But I felt batt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Awesome Performance 🔥🔥🔥🔥🔥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>4</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>best camera\\nbest os\\nlightning fast speed\\nlo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings             Review Summary  \\\n",
       "0        5                     Super!   \n",
       "1        5  Absolutely Awesome device   \n",
       "2        5             Really Awesome   \n",
       "3        5             Simply awesome   \n",
       "4        5           Perfect product!   \n",
       "..     ...                        ...   \n",
       "95       5         Highly recommended   \n",
       "96       5                  Fabulous!   \n",
       "97       5                  Very Good   \n",
       "98       2      Mind-blowing purchase   \n",
       "99       4         Highly recommended   \n",
       "\n",
       "                                          Full Review  \n",
       "0                                        For 30k nice  \n",
       "1                                         Superb!!!!!  \n",
       "2                                                Nice  \n",
       "3                                Very satisfy product  \n",
       "4                            Am facing heating issues  \n",
       "..                                                ...  \n",
       "95                                             Superb  \n",
       "96               Very nice phone.. thanks to flipcart  \n",
       "97  Good mobile with great things. But I felt batt...  \n",
       "98                          Awesome Performance 🔥🔥🔥🔥🔥  \n",
       "99  best camera\\nbest os\\nlightning fast speed\\nlo...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Ratings':ratings, 'Review Summary':review_summaries,'Full Review':full_reviews})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba9a74db",
   "metadata": {},
   "source": [
    "Q6: Scrape data forfirst 100 sneakers you find whenyou visit flipkart.com and search for “sneakers” inthe\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "9f3fb592",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "deee7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_input = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input\")\n",
    "\n",
    "location_input.send_keys('sneakers')\n",
    "\n",
    "#Now inspect search button find the class name & click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18acfc33",
   "metadata": {},
   "source": [
    "brand_tags = driver.find_elements(By.XPATH, '//div[@class=\"_2WkVRV\"]')  # Adjusted XPath to target only the rating element\n",
    "brands = []\n",
    "\n",
    "for tag in brand_tags:\n",
    "    brand = tag.text.split()[0]  # Splitting the text and extracting the rating\n",
    "    brands.append(brand)\n",
    "\n",
    "print(brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "b446da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BRUTON', 'BRUTON', 'Deals4you', 'URBANBOX', 'PUMA', 'BRUTON', 'BRUTON', 'BRUTON', 'Shoe', 'RED', 'BRUTON', 'BRUTON', 'U.S.', 'Layasa', 'UNDER', 'RED', 'Aeonik', 'BRUTON', 'aadi', 'Shoe', 'BRUTON', 'Layasa', 'PUMA', 'RED', 'HRX', 'HOTSTYLE', 'WHITE', 'WHITE', 'WOODLAND', 'PUMA', 'ATOM', 'WAAN', 'kardam&sons', 'aadi', 'Free', 'BIRDE', 'aadi', 'BRUTON', 'Robbie', 'PUMA', 'BRUTON', 'BRUTON', 'U.S.', 'Deals4you', 'ADIDAS', 'aadi', 'BRUTON', 'BRUTON', 'REEBOK', 'PUMA', 'BRUTON', 'BRUTON', 'PUMA', 'PUMA', 'PUMA', 'HOTSTYLE', 'Aeonik', 'BRUTON', 'RED', 'U.S.', 'Layasa', 'BRUTON', 'U.S.', 'PUMA', 'ATOM', 'PUMA', 'WAAN', 'ATOM', 'aadi', 'PUMA', 'WHITE', 'WHITE', 'aadi', 'ATOM', 'PUMA', 'Abros', 'World', 'aadi', 'CLYMB', 'Asics', 'BRUTON', 'BRUTON', 'PUMA', 'Abros', 'PUMA', 'HRX', 'Layasa', 'WAAN', 'DUCATI', 'PUMA', 'Layasa', 'WHITE', 'PUMA', 'PUMA', 'ATOM', 'U.S.', 'aadi', 'aadi', 'Sparx', 'PUMA', 'Layasa', 'Layasa', 'PUMA', 'PUMA', 'HRX', 'PUMA', 'Layasa', 'Footox', 'U.S.', 'PUMA', 'Footox', 'MEGPAR', 'PUMA', 'UNDER', 'BRUTON', 'TR', 'Bucik', 'Layasa', 'Layasa', 'UNDER']\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store brands\n",
    "brands = []\n",
    "\n",
    "# Set the start and end pages\n",
    "start_page = 0\n",
    "end_page = 11  # Adjusted to scrape 100 records\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(start_page, end_page):\n",
    "    # Find all brand elements on the current page\n",
    "    brand_tags = driver.find_elements(By.XPATH, '//div[@class=\"_2WkVRV\"]')\n",
    "\n",
    "    # Scraping brands from the current page\n",
    "    for tag in brand_tags:\n",
    "        brand = tag.text.split()[0]  # Splitting the text and extracting the brand\n",
    "        brands.append(brand)\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if len(brands) >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped brands\n",
    "print(brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "44695728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', 'Combo', 'Blktop', 'Rickie', 'Sneakers', 'black', 'Rideric', 'Sneakers', 'Stylish', 'Smash', 'Court', 'Sneakers', 'MONTON', 'Mesh', 'Stylish', 'Pacer', 'Stylish', 'Stylish', 'Trinity', 'Mirage', 'CLUB', 'Sneakers', 'Casual', 'BRENTT', 'knit', 'Sneakers', 'Modern', 'Casual', 'Casual', 'Sneakers', 'Sneakers']\n"
     ]
    }
   ],
   "source": [
    "ProductDescription_tags = driver.find_elements(By.XPATH, '//a[@class=\"IRpwTa\"]')  # Adjusted XPath to target only the rating element\n",
    "ProductDescription = []\n",
    "\n",
    "for tag in ProductDescription_tags:\n",
    "    product = tag.text.split()[0]  # Splitting the text and extracting the rating\n",
    "    ProductDescription.append(product)\n",
    "\n",
    "print(ProductDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698841a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "606f3e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Combo Sneaker Shoes Sneakers For Men\n",
      "Combo pack of 2 Trendy Men’s Casual Shoes Sneakers For ...\n",
      "Blktop Rider Preppy Sneakers For Men\n",
      "Rickie Sneakers For Men\n",
      "Sneakers For Women\n",
      "black shoes for men and boys Sneakers For Men\n",
      "Rideric Sneakers For Men\n",
      "Sneakers For Women\n",
      "Stylish & Trending Lifestyle Casual Sneakers For Men\n",
      "Smash V2 Buck Sneakers For Men\n",
      "Court Ultra Lite Sneakers For Men\n",
      "Sneakers For Men\n",
      "MONTON 5.0 Sneakers For Men\n",
      "Mesh |Lightweight|Comfort|Summer|Trendy|Walking|Outdoor...\n",
      "Stylish and Comfirtable Shoes For Mens Sneakers For Men\n",
      "Pacer Future Web Sneakers For Men\n",
      "Stylish Casual Sports Shoe Sneakers For Women\n",
      "Stylish Casual Sports Shoe Sneakers Sneakers For Women\n",
      "Trinity Lite Sneakers For Men\n",
      "Mirage Sport Tech Sneakers For Men\n",
      "CLUB CULTURE Sneakers For Men\n",
      "Sneakers For Women\n",
      "Casual Shoes Sneakers For Women\n",
      "BRENTT 2.0 Sneakers For Men\n",
      "knit V3 Sneakers For Men\n",
      "Sneakers For Men\n",
      "Modern Trendy Sneakers Shoes Sneakers For Men\n",
      "Casual Shoes| Caual Sneakers| White shoes| White Sneake...\n",
      "Casual Sneakers White Shoes For Girls And Sneakers For ...\n",
      "Sneakers For Women\n",
      "Sneakers For Men\n",
      "Combo Pack Of 2 Casual Shoes Sneakers For Men\n",
      "Combo pack of 2 Trendy Sneaker Shoes Walking Shoes Snea...\n",
      "BMW MMS re schooltream Sneakers For Men\n",
      "Sneakers For Men Sneakers For Men\n",
      "Combo Pack Of 2 Casual Shoes Sneakers For Men\n",
      "Combo Pack Of 2 Casual Shoes for mens Sneakers For Men\n",
      "Modern Trendy Sneakers Shoes Sneakers For Men\n",
      "Sneakers For Men\n",
      "Combo Pack Of 2 Casual Shoes Sneakers For Men\n",
      "Modern Trendy Sneakers Shoes Sneakers For Men\n",
      "WINSTON-N Sneakers For Men\n",
      "Sneakers For Men Sneakers For Men\n",
      "Rbd Game Low Sneakers For Men\n",
      "BRENTT 2.0 Sneakers For Men\n",
      "Stylish Casual Sports Shoe Sneakers Sneakers For Women\n",
      "2 Combo Sneaker Shoes Sneakers For Men\n",
      "Archfitglide-Step-HI Sneakers For Men\n",
      "Sneakers For Men\n",
      "Combo Pack Of 2 Casual Shoes Sneakers For Men\n",
      "Stylish and Comfirtable Shoes For Mens Sneakers For Men\n",
      "Future Rider Override Sneakers For Men\n",
      "Stylish & Trending Outdoor Walking Comfortable Sneakers...\n",
      "Stylish & Trending Outdoor Walking Comfortable Sneakers...\n",
      "OG-D1 Sneakers For Men\n",
      "CLUB CULTURE Sneakers For Men\n",
      "Alpha Predator Sneakers For Men\n",
      "Synthetic Leather |Lightweight|Comfort|Summer|Trendy|Wa...\n",
      "Mono Knit X Sneakers For Men\n",
      "Sneakers For Men\n",
      "Sneakers For Men\n",
      "Modern Trendy Sneakers Shoes Sneakers For Men\n",
      "ST Runner v3 L Sneakers For Men\n",
      "Sneakers For Men\n",
      "2 Combo Sneaker Shoes Sneakers For Men\n",
      "Combo pack of 2 Trendy Men’s Casual Shoes Sneakers For ...\n",
      "JAXEN-2E Sneakers For Men\n",
      "Gansta1 Sneakers For Men\n",
      "Stylish Casual Sports Shoe Sneakers Sneakers For Women\n",
      "373 Sneakers For Men\n",
      "Combo Pack of 2 Casual Shoes Sneakers For Men\n",
      "Stylish Casual Sports Shoe Sneakers For Women\n",
      "Casual Sneakers White Shoes For Girls And Sneakers For ...\n",
      "Jasper-18 White Sneakers,Casuals,Loafers,Stylish Sneake...\n",
      "Alpha Predator Sneakers For Men\n",
      "Sneakers For Women\n",
      "Stylish & Trending Lifestyle Comfortable Casual Sneaker...\n",
      "Rerooted Classics Sneakers For Men\n",
      "Stylish & Trending Lifestyle Casual Sneakers For Men\n",
      "Sneakers For Women\n",
      "level Sneakers For Men\n",
      "Graviton Sneakers For Men\n",
      "Pacer Future Trail Sneakers For Men\n",
      "Stylish & Trending Outdoor Walking Comfortable Sneakers...\n",
      "Sneakers For Men\n",
      "CLARKIN 3.0 Sneakers For Men\n",
      "White Stylish New Trend Casual Sneakers For Women\n",
      "Hustle V2 Sneakers For Men\n",
      "Ferrari Future Cat Ultra Sneakers For Men\n",
      "Fashion and Stylish Soft Lace Up Sneakers Causal Shoes ...\n",
      "Sneakers For Men\n",
      "Women's Mesh Sports Shoes Walking Sneakers For Women\n",
      "Sneakers For Men\n",
      "first-4-Black White Sneakers Shoes,Loafers,Casuals Snea...\n",
      "HIGH HEEL SHOES Sneakers For Women\n",
      "Casual sneaker Height Enhancement shoes Sneakers For Me...\n",
      "FLYING FURY Sneakers For Men\n",
      "Boston-01 Chunky Sneakers,Loafers,Walking Shoes Sneaker...\n",
      "Sneaker Casual Shoes for Men | Soft Cushioned Sneakers ...\n",
      "Go Walk Joy-Daring Sneakers For Women\n",
      "Levi's Men's Lancer Sneakers Sneakers For Men\n",
      "Premium Stylish Comfortable Shoe for Girls and Womens S...\n",
      "Sneakers For Women\n",
      "Troy MU Sneakers For Men\n",
      "Club 5v5 Sneakers For Men\n",
      "JAPAN S Sneakers For Men\n",
      "Sneakers For Women\n",
      "Sneakers For Women\n",
      "990 Sneakers For Men\n",
      "Pacer Future Web Sneakers For Men\n",
      "Thunder-03 Loafers,Casuals,Stylish with Extra Comfort S...\n",
      "Sneakers For Men\n",
      "GO RUN PERSISTENCE Sneakers For Men\n",
      "CA Pro Mid Sneakers For Men\n",
      "Sneakers For Men\n",
      "Picaaso JR 07 | Casual Sneakers Shoes | Sneakers for Me...\n",
      "Sneakers For Men\n",
      "Comfortable Outdoor,Casual, walking, Training,Trekking ...\n",
      "Running Shoes,Sports Shoes for Women|Memory Foam Insole...\n",
      "Picaaso JR 07 | Casual Sneakers Shoes | Sneakers for Me...\n",
      "Cassia Sneakers For Women\n",
      "996 Sneakers For Men\n",
      "Casual Shoes Sneakers For Women\n",
      "Big Fox Croco Sole Sneakers For Men\n",
      "BIRDIE Sneakers For Women\n",
      "Oricum Casual Shoes For Men Walking,Sneakers ,Loafers s...\n",
      "Sneakers For Women\n"
     ]
    }
   ],
   "source": [
    "ProductDescriptions = []\n",
    "\n",
    "# Set the start and end pages\n",
    "start_page = 0\n",
    "end_page = 11  # Adjusted to scrape 100 records\n",
    "\n",
    "# Loop through each page\n",
    "for page in range(start_page, end_page):\n",
    "    # Find all product description elements on the current page\n",
    "    ProductDescription_tags = driver.find_elements(By.XPATH, '//a[@class=\"IRpwTa\"]')\n",
    "\n",
    "    # Scraping and formatting product descriptions from the current page\n",
    "    for tag in ProductDescription_tags:\n",
    "        # Get the text content of the element\n",
    "        product_description = tag.text\n",
    "        # Append the formatted product description to the list\n",
    "        ProductDescriptions.append(product_description)\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if len(ProductDescriptions) >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped and formatted product descriptions\n",
    "for description in ProductDescriptions:\n",
    "    print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9a712b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['₹399', '₹1,199', '66%', '₹798', '₹899', '11%', '₹3,419', '₹5,999', '43%', '₹579', '₹999', '42%', '₹454', '₹2,999', '84%', '₹1,095', '₹1,699', '35%', '₹817', '₹999', '18%', '₹1,299', '₹2,999', '56%', '₹3,419', '₹5,999', '43%', '₹1,749', '₹3,199', '45%', '₹589', '₹999', '41%', '₹386', '₹999', '61%', '₹1,599', '₹3,999', '60%', '₹3,200', '₹6,999', '54%', '₹3,643', '₹5,999', '39%', '₹399', '₹999', '60%', '₹499', '₹1,999', '75%', '₹380', '₹1,499', '74%', '₹13,200', '₹29,999', '55%', '₹2,099', '₹5,999', '65%', '₹767', '₹899', '14%', '₹1,125', '₹2,299', '51%', '₹6,049', '₹10,999', '45%', '₹4,299', '₹9,999', '57%', '₹962', '₹1,299', '25%', '₹1,934', '₹4,499', '57%', '₹799', '₹4,999', '84%', '₹809', '₹1,899', '57%', '₹704', '₹998', '29%', '₹3,149', '₹8,999', '65%', '₹689', '₹1,199', '42%', '₹799', '₹4,999', '84%', '₹3,219', '₹6,999', '54%', '₹5,808', '₹13,999', '58%', '₹429', '₹999', '57%', '₹399', '₹999', '60%', '₹649', '₹2,245', '71%', '₹1,099', '₹1,799', '38%', '₹690', '₹998', '30%', '₹498', '₹999', '50%']\n"
     ]
    }
   ],
   "source": [
    "Price_tags = driver.find_elements(By.XPATH, '//a[@class=\"_3bPFwb\"]/div/div')  # Adjusted XPath to target only the rating element\n",
    "PriceList = []\n",
    "\n",
    "for tag in Price_tags:\n",
    "    price = tag.text.split()[0]  # Splitting the text and extracting the rating\n",
    "    PriceList.append(price)\n",
    "\n",
    "print(PriceList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f3b390a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brands</th>\n",
       "      <th>Discription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹1,199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deals4you</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>URBANBOX</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>TR</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Bucik</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>₹999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>UNDER</td>\n",
       "      <td>Sneakers For Women</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Brands         Discription   Price\n",
       "0       BRUTON  Sneakers For Women    ₹399\n",
       "1       BRUTON  Sneakers For Women  ₹1,199\n",
       "2    Deals4you  Sneakers For Women     66%\n",
       "3     URBANBOX  Sneakers For Women    ₹798\n",
       "4         PUMA  Sneakers For Women    ₹899\n",
       "..         ...                 ...     ...\n",
       "115         TR  Sneakers For Women    ₹998\n",
       "116      Bucik  Sneakers For Women     30%\n",
       "117     Layasa  Sneakers For Women    ₹498\n",
       "118     Layasa  Sneakers For Women    ₹999\n",
       "119      UNDER  Sneakers For Women     50%\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Brands':brands, 'Discription':description,'Price':PriceList})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88a5dfa0",
   "metadata": {},
   "source": [
    "#20 records aded previously"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2357aaf8",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "41ba37c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening flipcart Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "09e27487",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_input = driver.find_element(By.XPATH, \"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input\")\n",
    "\n",
    "location_input.send_keys('Laptop')\n",
    "\n",
    "#Now inspect search button find the class name & click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "76dbe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "Processor = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div[1]/div[1]/div/div[1]/div/section[5]/div[2]/div[1]/div[4]/div/label/div[1]\")\n",
    "\n",
    "Processor.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tags = driver.find_elements(By.XPATH, '//div[@class=\"_3pLy-c row\"]/div/div')  # Adjusted XPath to target only the rating element\n",
    "Title_list = []\n",
    "\n",
    "for tag in title_tags:\n",
    "    title = tag.text  # Splitting the text and extracting the rating\n",
    "    Title_list.append(title)\n",
    "\n",
    "print(Title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e0e7e46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DELL Alienware Intel Core i7 12th Gen 12700H - (16 GB/1 TB SSD/Windows 11 Home/6 GB Graphics/NVIDIA Ge...\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR5 RAM\n",
      "64 bit Windows 11 Operating System\n",
      "1 TB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Onsite Premium Support Plus (Includes ADP)\n",
      "₹1,79,483\n",
      "₹3,20,92644% off\n",
      "\n",
      "ASUS TUF Gaming F15 (2023) with 90WHr Battery Intel H-Series Intel Core i7 12th Gen 12700H - (16 GB/51...\n",
      "4.5164 Ratings & 14 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹99,990\n",
      "₹1,33,99025% off\n",
      "\n",
      "Infinix X3 Slim Intel Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home) XL422 Thin and...\n",
      "4.375 Ratings & 12 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB LPDDR4X RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹44,990\n",
      "₹69,99035% off\n",
      "\n",
      "ASUS Vivobook S15 OLED Intel EVO H-Series Intel Core i7 12th Gen 12700H - (16 GB/512 GB SSD/Windows 11...\n",
      "478 Ratings & 7 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹77,990\n",
      "₹1,16,99033% off\n",
      "SAMSUNG Galaxy Book3 Intel Core i7 13th Gen 1355U - (16 GB/512 GB SSD/Windows 11 Home) NP750XFG-KA3IN ...\n",
      "4.5302 Ratings & 40 Reviews\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB LPDDR4X RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹86,990\n",
      "₹1,12,99023% off\n",
      "\n",
      "ZEBRONICS Pro Series Z Intel Core i7 12th Gen 1255U - (16 GB/1 TB SSD/Windows 11 Home) ZEB-NBC 5S Lapt...\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "1 TB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Carry-in Warranty\n",
      "₹52,990\n",
      "₹81,99935% off\n",
      "\n",
      "ZEBRONICS Pro Series Z Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home) ZEB-NBC 5S La...\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Carry-in Warranty\n",
      "₹46,990\n",
      "₹78,99940% off\n",
      "\n",
      "DELL Latitude 14 Intel Core i7 11th Gen - (16 GB/512 GB SSD/Windows 11 Pro) Latitude 3420 Business Lap...\n",
      "Intel Core i7 Processor (11th Gen)\n",
      "16 GB DDR4 RAM\n",
      "64 bit Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 inch) Display\n",
      "3-Year ADP Warranty\n",
      "₹99,990\n",
      "₹1,35,00025% off\n",
      "\n",
      "WINGS Nuvobook Pro Aluminium Alloy Metal Body Intel Intel Core i7 11th Gen 1165G7 - (16 GB/512 GB SSD/...\n",
      "4.185 Ratings & 21 Reviews\n",
      "Intel Core i7 Processor (11th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹49,990\n",
      "₹89,99944% off\n",
      "\n",
      "ZEBRONICS Pro Series Z Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home) ZEB-NBC 5S Th...\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 inch) Display\n",
      "1 Year Carry-in Warranty\n",
      "₹46,990\n",
      "₹78,99940% off\n",
      "\n",
      "HP Pavilion x360 Intel Core i7 13th Gen - (16 GB/512 GB SSD/Windows 11 Home) 14-ek1020TU 2 in 1 Laptop\n",
      "4.36 Ratings & 2 Reviews\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 inch) Touchscreen Display\n",
      "1 year Manufacturer\n",
      "₹89,990\n",
      "₹1,04,04813% off\n",
      "\n",
      "HP Victus Intel Core i7 12th Gen 12650H - (16 GB/512 GB SSD/Windows 11 Home/4 GB Graphics/NVIDIA GeFor...\n",
      "4.422 Ratings & 2 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "64 bit Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹82,777\n",
      "₹1,12,90026% off\n",
      "\n",
      "DELL Inspiron Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home/2 GB Graphics) Inspiron...\n",
      "4.130 Ratings & 2 Reviews\n",
      "Processor: Intel i7-1255U (Base- 3.50 GHz & Turbo up to 4.70 GHz) 10 Cores\n",
      "RAM & Storage: 16GB DDR4 & 512GB SSD\n",
      "Graphics & Keyboard: NVIDIAGeForceMX550 (2GB GDDR6) + Fingerprint Reader\n",
      "Display: 14.0\" FHD+ WVA Truelife Touch Narrow Border 250 nits, Dell Active Pen\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "64 bit Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 Inch) Touchscreen Display\n",
      "1 Year Onsite Hardware Service\n",
      "₹84,990\n",
      "₹1,29,89034% off\n",
      "\n",
      "Infinix Y4 Max Series Intel Core i7 13th Gen 1355U - (16 GB/512 GB SSD/Windows 11 Home) YL613 Thin and...\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB LPDDR4X RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "40.64 cm (16 inch) Display\n",
      "NA\n",
      "1 Year Onsite Warranty\n",
      "₹52,990\n",
      "₹79,99033% off\n",
      "\n",
      "Infinix INBook X2 Plus Intel Core i7 11th Gen 1195G7 - (16 GB/512 GB SSD/Windows 11 Home) XL25 Thin an...\n",
      "478 Ratings & 17 Reviews\n",
      "Intel Core i7 Processor (11th Gen)\n",
      "16 GB LPDDR4X RAM\n",
      "64 bit Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹57,990\n",
      "₹79,99027% off\n",
      "\n",
      "MSI Katana 15 Intel Core i7 13th Gen 13620H - (16 GB/1 TB SSD/Windows 11 Home/6 GB Graphics/NVIDIA GeF...\n",
      "4.17 Ratings & 1 Reviews\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB DDR5 RAM\n",
      "Windows 11 Operating System\n",
      "1 TB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "2 Year Carry-in Warranty\n",
      "₹1,09,990\n",
      "₹1,54,99029% off\n",
      "\n",
      "HP 15s (2023) Intel Core i7 13th Gen 1360P - (16 GB/512 GB SSD/Windows 11 Home) 15-fd0024TU Thin and L...\n",
      "4.2161 Ratings & 17 Reviews\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB DDR4 RAM\n",
      "64 bit Windows 11 Operating System\n",
      "512 GB SSD\n",
      "39.62 cm (15.6 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹77,990\n",
      "₹91,14114% off\n",
      "\n",
      "SAMSUNG Galaxy Book3 Pro EVO AMOLED Intel Core i7 13th Gen 1360P - (16 GB/1 TB SSD/Windows 11 Home) NP...\n",
      "Intel Core i7 Processor (13th Gen)\n",
      "16 GB DDR5 RAM\n",
      "Windows 11 Operating System\n",
      "1 TB SSD\n",
      "40.64 cm (16 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹1,50,990\n",
      "₹1,83,25417% off\n",
      "\n",
      "ZEBRONICS Pro Series Z Intel Core i7 12th Gen 1255U - (16 GB/1 TB SSD/Windows 11 Home) ZEB-NBC 5S Thin...\n",
      "410 Ratings & 1 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB DDR4 RAM\n",
      "Windows 11 Operating System\n",
      "1 TB SSD\n",
      "39.62 cm (15.6 inch) Display\n",
      "1 Year Carry-in Warranty\n",
      "₹52,990\n",
      "₹81,99935% off\n",
      "\n",
      "Infinix X3 Slim Intel Intel Core i7 12th Gen 1255U - (16 GB/512 GB SSD/Windows 11 Home) XL422 Thin and...\n",
      "4.375 Ratings & 12 Reviews\n",
      "Intel Core i7 Processor (12th Gen)\n",
      "16 GB LPDDR4X RAM\n",
      "Windows 11 Operating System\n",
      "512 GB SSD\n",
      "35.56 cm (14 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹44,990\n",
      "₹69,99035% off\n",
      "Acer Predator Helios 300 Intel Core i7 11th Gen 11800H - (16 GB/1 TB SSD/Windows 10 Home/6 GB Graphics...\n",
      "4.362 Ratings & 9 Reviews\n",
      "Intel Core i7 Processor (11th Gen)\n",
      "16 GB DDR4 RAM\n",
      "64 bit Windows 10 Operating System\n",
      "1 TB SSD\n",
      "39.62 cm (15.6 inches) Display\n",
      "Acer Care Center, Acer Product Registration, Planet 9, Predator Sense\n",
      "1 Year International Travelers Warranty (ITW)\n",
      "₹97,695\n",
      "₹1,49,99034% off\n",
      "\n",
      "HP OMEN Intel Core i7 14th Gen 14700HX - (16 GB/1 TB SSD/Windows 11 Home/8 GB Graphics/NVIDIA GeForce ...\n",
      "Intel Core i7 Processor (14th Gen)\n",
      "16 GB DDR5 RAM\n",
      "Windows 11 Operating System\n",
      "1 TB SSD\n",
      "40.89 cm (16.1 Inch) Display\n",
      "1 Year Onsite Warranty\n",
      "₹1,75,721\n",
      "₹1,97,50711% off\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Title_list = []  # Initialize the list to store titles\n",
    "records_scraped = 0  # Initialize a counter for the number of records scraped\n",
    "\n",
    "# Loop until 100 records are scraped\n",
    "while records_scraped < 100:\n",
    "    # Find all title elements on the current page\n",
    "    title_tags = driver.find_elements(By.XPATH, '//div[@class=\"_3pLy-c row\"]/div/div')\n",
    "\n",
    "    # Scraping titles from the current page\n",
    "    for tag in title_tags:\n",
    "        # Get the text content of the element\n",
    "        title = tag.text\n",
    "        # Append the title to the list\n",
    "        Title_list.append(title)\n",
    "        # Increment the counter\n",
    "        records_scraped += 1\n",
    "        # Break the loop if 100 records are scraped\n",
    "        if records_scraped >= 100:\n",
    "            break\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if records_scraped >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped titles\n",
    "for title in Title_list:\n",
    "    print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "587e142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.5', '4.3', '4', '4.5', '4.1', '4.3', '4.4', '4.1', '4', '4.1', '4.2', '4', '4.3', '4.3', '4.4', '4.2']\n"
     ]
    }
   ],
   "source": [
    "Ratings_tags = driver.find_elements(By.XPATH, '//div[@class=\"gUuXy-\"]/span/div')  # Adjusted XPath to target only the rating element\n",
    "Ratings_list = []\n",
    "\n",
    "for tag in Ratings_tags:\n",
    "    rating = tag.text  # Splitting the text and extracting the rating\n",
    "    Ratings_list.append(rating)\n",
    "\n",
    "print(Ratings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7edbc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "//div[@class=\"_3_L3jD\"]/div/span/div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "19eef98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5\n",
      "4.3\n",
      "4\n",
      "4.5\n",
      "4.1\n",
      "4.3\n",
      "4.4\n",
      "4.1\n",
      "4\n",
      "4.1\n",
      "4.2\n",
      "4\n",
      "4.3\n",
      "4.3\n",
      "4.4\n",
      "4.2\n",
      "4.4\n",
      "4.5\n",
      "4.1\n",
      "4.4\n",
      "4.6\n",
      "4.3\n",
      "4\n",
      "4.2\n",
      "4.3\n",
      "4.2\n",
      "4.4\n",
      "4.4\n",
      "4.5\n",
      "4.3\n",
      "4.2\n",
      "4.7\n",
      "3.6\n",
      "3.6\n",
      "4.4\n",
      "4.5\n",
      "4.3\n",
      "4\n",
      "4.5\n",
      "4.1\n",
      "4.3\n",
      "4.4\n",
      "4.1\n",
      "4.1\n",
      "4\n",
      "4.2\n",
      "4\n",
      "4.3\n",
      "4.3\n",
      "4.4\n",
      "4.2\n",
      "4.4\n",
      "4.5\n",
      "4.1\n",
      "4.4\n",
      "4.6\n",
      "4.3\n",
      "4\n",
      "4.2\n",
      "4.3\n",
      "4.2\n",
      "4.4\n",
      "4.4\n",
      "4.5\n",
      "4.3\n",
      "4.2\n",
      "4.7\n",
      "3.6\n",
      "3.6\n",
      "4.4\n",
      "4.5\n",
      "4.3\n",
      "4\n",
      "4.5\n",
      "4.1\n",
      "4.3\n",
      "4.4\n",
      "4.1\n",
      "4.1\n",
      "4\n",
      "4.2\n",
      "4\n",
      "4.3\n",
      "4.3\n",
      "4.4\n",
      "4.2\n",
      "4.4\n",
      "4.5\n",
      "4.1\n",
      "4.4\n",
      "4.6\n",
      "4.3\n",
      "4\n",
      "4.2\n",
      "4.3\n",
      "4.2\n",
      "4.4\n",
      "4.4\n",
      "4.5\n",
      "4.3\n"
     ]
    }
   ],
   "source": [
    "Ratings_list = []  # Initialize the list to store ratings\n",
    "records_scraped = 0  # Initialize a counter for the number of records scraped\n",
    "\n",
    "# Loop until 100 records are scraped\n",
    "while records_scraped < 100:\n",
    "    # Find all rating elements on the current page\n",
    "    Ratings_tags = driver.find_elements(By.XPATH, '//div[@class=\"gUuXy-\"]/span/div')\n",
    "\n",
    "    # Scraping ratings from the current page\n",
    "    for tag in Ratings_tags:\n",
    "        # Get the text content of the element\n",
    "        rating = tag.text\n",
    "        # Append the rating to the list\n",
    "        Ratings_list.append(rating)\n",
    "        # Increment the counter\n",
    "        records_scraped += 1\n",
    "        # Break the loop if 100 records are scraped\n",
    "        if records_scraped >= 100:\n",
    "            break\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if records_scraped >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]/span')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped ratings\n",
    "for rating in Ratings_list:\n",
    "    print(rating)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e2eafb7",
   "metadata": {},
   "source": [
    "Price_tags = driver.find_elements(By.XPATH, '//div[@class=\"_25b18c\"]/div')  # Adjusted XPath to target only the rating element\n",
    "Price_tags_list = []\n",
    "\n",
    "for tag in Price_tags:\n",
    "    price = tag.text  # Splitting the text and extracting the rating\n",
    "    Price_tags_list.append(price)\n",
    "\n",
    "print(Price_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "984ab4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "₹99,990\n",
      "₹1,39,990\n",
      "28% off\n",
      "₹86,990\n",
      "₹1,12,990\n",
      "23% off\n",
      "₹57,990\n",
      "₹1,08,000\n",
      "46% off\n",
      "₹90,990\n",
      "₹1,81,999\n",
      "50% off\n",
      "₹73,990\n",
      "₹1,04,990\n",
      "29% off\n",
      "₹77,990\n",
      "₹1,16,990\n",
      "33% off\n",
      "₹96,867\n",
      "₹1,45,990\n",
      "33% off\n",
      "₹74,990\n",
      "₹1,04,990\n",
      "28% off\n",
      "₹59,990\n",
      "₹99,990\n",
      "40% off\n",
      "₹64,990\n",
      "₹85,990\n",
      "24% off\n",
      "₹86,341\n",
      "₹1,02,917\n",
      "16% off\n",
      "₹1,14,990\n",
      "₹1,44,999\n",
      "20% off\n",
      "₹52,990\n",
      "₹79,990\n",
      "33% off\n",
      "₹1,32,990\n",
      "₹1,53,446\n",
      "13% off\n",
      "₹69,360\n",
      "₹1,00,168\n",
      "30% off\n",
      "₹1,11,990\n",
      "₹1,35,990\n",
      "17% off\n",
      "₹46,990\n",
      "₹78,999\n",
      "40% off\n",
      "₹66,500\n",
      "₹82,990\n",
      "19% off\n",
      "₹1,49,990\n",
      "₹2,32,314\n",
      "35% off\n",
      "₹79,839\n",
      "₹99,000\n",
      "19% off\n",
      "₹1,69,990\n",
      "₹1,98,710\n",
      "14% off\n",
      "₹68,990\n",
      "₹79,990\n",
      "13% off\n",
      "₹47,990\n",
      "₹99,000\n",
      "51% off\n",
      "₹94,990\n",
      "₹1,23,999\n",
      "23% off\n",
      "₹1,79,483\n",
      "₹3,20,926\n",
      "44% off\n",
      "₹99,990\n",
      "₹1,33,990\n",
      "25% off\n",
      "₹44,990\n",
      "₹69,990\n",
      "35% off\n",
      "₹77,990\n",
      "₹1,16,990\n",
      "33% off\n",
      "₹86,990\n",
      "₹1,12,990\n",
      "23% off\n",
      "₹52,990\n",
      "₹81,999\n",
      "35% off\n",
      "₹46,990\n",
      "₹78,999\n",
      "40% off\n",
      "₹99,990\n",
      "₹1,35,000\n",
      "25% off\n",
      "₹46,990\n",
      "₹78,999\n",
      "40% off\n",
      "₹49,990\n"
     ]
    }
   ],
   "source": [
    "Price_tags_list = []  # Initialize the list to store prices\n",
    "records_scraped = 0  # Initialize a counter for the number of records scraped\n",
    "\n",
    "# Loop until 100 records are scraped\n",
    "while records_scraped < 100:\n",
    "    # Find all price elements on the current page\n",
    "    Price_tags = driver.find_elements(By.XPATH, '//div[@class=\"_25b18c\"]/div')\n",
    "\n",
    "    # Scraping prices from the current page\n",
    "    for tag in Price_tags:\n",
    "        # Get the text content of the element\n",
    "        price = tag.text\n",
    "        # Append the price to the list\n",
    "        Price_tags_list.append(price)\n",
    "        # Increment the counter\n",
    "        records_scraped += 1\n",
    "        # Break the loop if 100 records are scraped\n",
    "        if records_scraped >= 100:\n",
    "            break\n",
    "\n",
    "    # Check if 100 records have been scraped\n",
    "    if records_scraped >= 100:\n",
    "        break\n",
    "\n",
    "    # Click on the \"Next\" button to navigate to the next page\n",
    "    next_button = driver.find_element(By.XPATH, '//a[@class=\"_1LKTO3\"]/span')\n",
    "    next_button.click()\n",
    "\n",
    "    # Adding a small delay to ensure the next page is loaded before scraping\n",
    "    time.sleep(3)\n",
    "\n",
    "# Print the scraped prices\n",
    "for price in Price_tags_list:\n",
    "    print(price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "e5157561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(Title_list),len(Ratings_list),len(Price_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "ab64c7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title &amp; Discription</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELL Alienware Intel Core i7 12th Gen 12700H -...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹99,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intel Core i7 Processor (12th Gen)\\n16 GB DDR5...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹1,39,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>₹1,79,483\\n₹3,20,92644% off</td>\n",
       "      <td>4</td>\n",
       "      <td>28% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASUS TUF Gaming F15 (2023) with 90WHr Battery ...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>₹1,12,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td></td>\n",
       "      <td>4.2</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>HP OMEN Intel Core i7 14th Gen 14700HX - (16 G...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹46,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Intel Core i7 Processor (14th Gen)\\n16 GB DDR5...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹78,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>₹1,75,721\\n₹1,97,50711% off</td>\n",
       "      <td>4.5</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹49,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title & Discription Ratings      Price\n",
       "0   DELL Alienware Intel Core i7 12th Gen 12700H -...     4.5    ₹99,990\n",
       "1   Intel Core i7 Processor (12th Gen)\\n16 GB DDR5...     4.3  ₹1,39,990\n",
       "2                         ₹1,79,483\\n₹3,20,92644% off       4    28% off\n",
       "3                                                         4.5    ₹86,990\n",
       "4   ASUS TUF Gaming F15 (2023) with 90WHr Battery ...     4.1  ₹1,12,990\n",
       "..                                                ...     ...        ...\n",
       "95                                                        4.2    25% off\n",
       "96  HP OMEN Intel Core i7 14th Gen 14700HX - (16 G...     4.4    ₹46,990\n",
       "97  Intel Core i7 Processor (14th Gen)\\n16 GB DDR5...     4.4    ₹78,999\n",
       "98                        ₹1,75,721\\n₹1,97,50711% off     4.5    40% off\n",
       "99                                                        4.3    ₹49,990\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Title & Discription':Title_list, 'Ratings':Ratings_list,'Price':Price_tags_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147e1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "cb84628b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening flipcart Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.azquotes.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "79f439d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "//a[@class=\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "334a7094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The essence of strategy is choosing what not to do.', 'One cannot and must not try to erase the past merely because it does not fit the present.', 'Patriotism means to stand by the country. It does not mean to stand by the president.', 'Death is something inevitable. When a man has done what he considers to be his duty to his people and his country, he can rest in peace. I believe I have made that effort and that is, therefore, why I will sleep for the eternity.', 'You have to love a nation that celebrates its independence every July 4, not with a parade of guns, tanks, and soldiers who file by the White House in a show of strength and muscle, but with family picnics where kids throw Frisbees, the potato salad gets iffy, and the flies die from happiness. You may think you have overeaten, but it is patriotism.', 'Be more concerned with your character than your reputation, because your character is what you really are, while your reputation is merely what others think you are.', 'Weak people revenge. Strong people forgive. Intelligent People Ignore.', \"A mind is like a parachute. It doesn't work if it is not open.\", 'Never be afraid to raise your voice for honesty and truth and compassion against injustice and lying and greed. If people all over the world...would do this, it would change the earth.', 'There are three kinds of men. The one that learns by reading. The few who learn by observation. The rest of them have to pee on the electric fence for themselves.', \"A strong nation, like a strong person, can afford to be gentle, firm, thoughtful, and restrained. It can afford to extend a helping hand to others. It's a weak nation, like a weak person, that must behave with bluster and boasting and rashness and other signs of insecurity.\", 'The difference between stupidity and genius is that genius has its limits.', 'We the people are the rightful masters of both Congress and the courts, not to overthrow the Constitution but to overthrow the men who pervert the Constitution.', 'With or without religion, you would have good people doing good things and evil people doing evil things. But for good people to do evil things, that takes religion.', 'Human kindness has never weakened the stamina or softened the fiber of a free people. A nation does not have to be cruel to be tough.', 'A person who never made a mistake never tried anything new.', 'My mission in life is not merely to survive, but to thrive; and to do so with some passion, some compassion, some humor, and some style', 'Life is short, Break the Rules. Forgive quickly, Kiss slowly. Love truly. Laugh uncontrollably And never regret ANYTHING That makes you smile.', 'This country will not be a good place for any of us to live in unless we make it a good place for all of us to live in.', 'In a world filled with hate, we must still dare to hope. In a world filled with anger, we must still dare to comfort. In a world filled with despair, we must still dare to dream. And in a world filled with distrust, we must still dare to believe.', 'Darkness cannot drive out darkness; only light can do that. Hate cannot drive out hate; only love can do that.', 'To be a Christian means to forgive the inexcusable because God has forgiven the inexcusable in you.', 'A man who is good enough to shed his blood for the country is good enough to be given a square deal afterwards.', 'Kindness is the language which the deaf can hear and the blind can see.', 'Most folks are as happy as they make up their minds to be.', 'A happy marriage is the union of two good forgivers.', 'I believe in Christianity as I believe that the sun has risen: not only because I see it, but because by it I see everything else.', 'Politics is the art of looking for trouble, finding it everywhere, diagnosing it incorrectly and applying the wrong remedies.', \"I believe in everything until it's disproved. So I believe in fairies, the myths, dragons. It all exists, even if it's in your mind. Who's to say that dreams and nightmares aren't as real as the here and now?\", 'At the end of your life, you will never regret not having passed one more test, not winning one more verdict or not closing one more deal. You will regret time not spent with a husband, a friend, a child, or a parent.', 'I never think of the future - it comes soon enough.', \"Our greatest fear should not be of failure but of succeeding at things in life that don't really matter.\", 'Failure is simply the opportunity to begin again, this time more intelligently.', 'Music expresses that which cannot be put into words.', 'Good actions are a guard against the blows of adversity.', 'Positive thinking will let you do everything better than negative thinking will.', \"You have enemies? Good. That means you've stood up for something, sometime in your life.\", \"You don't have to be great to start, but you have to start to be great\", 'Let every nation know, whether it wishes us well or ill, that we shall pay any price, bear any burden, meet any hardship, support any friend, oppose any foe to assure the survival and the success of liberty.', \"If you want total security, go to prison. There you're fed, clothed, given medical care and so on. The only thing lacking... is freedom.\", 'Socialism is a philosophy of failure, the creed of ignorance, and the gospel of envy, its inherent virtue is the equal sharing of misery.', 'It is the supreme art of the teacher to awaken joy in creative expression and knowledge.', \"Once you replace negative thoughts with positive ones, you'll start having positive results.\", \"You build on failure. You use it as a stepping stone. Close the door on the past. You don't try to forget the mistakes, but you don't dwell on it. You don't let it have any of your energy, or any of your time, or any of your space.\", 'All my life through, the new sights of Nature made me rejoice like a child.', \"I've learned that people will forget what you said, people will forget what you did, but people will never forget how you made them feel.\", 'Marriage is not a ritual or an end. It is a long, intricate, intimate dance together and nothing matters more than your own sense of balance and your choice of partner.', 'When obstacles arise, you change your direction to reach your goal; you do not change your decision to get there.', 'Time you enjoy wasting, was not wasted.', 'The next time some academics tell you how important diversity is, ask how many Republicans there are in their sociology department.', 'Faith is not the belief that God will do what you want. It is the belief that God will do what is right.', \"If you can't fly then run, if you can't run then walk, if you can't walk then crawl, but whatever you do you have to keep moving forward.\", 'It is hard to imagine a more stupid or more dangerous way of making decisions than by putting those decisions in the hands of people who pay no price for being wrong.', 'I say that the most liberating thing about beauty is realizing that you are the beholder.', 'The ultimate measure of a man is not where he stands in moments of comfort and convenience, but where he stands at times of challenge and controversy.', 'The soldier, above all other people, prays for peace, for he must suffer and bear the deepest wounds and scars of war.', 'A true leader has the confidence to stand alone, the courage to make tough decisions, and the compassion to listen to the needs of others. He does not set out to be a leader, but becomes one by the equality of his actions and the integrity of his intent.', 'I am a Soldier, I fight where I am told, and I win where I fight.', \"Trying to do the Lord's work in your own strength is the most confusing, exhausting, and tedious of all work. But when you are filled with the Holy Spirit, then the ministry of Jesus just flows out of you.\", \"Sports for me is when a guy walks off the court, and you really can't tell whether he won or lost, when he carries himself with pride either way.\", 'Expect the best. Prepare for the worst. Capitalize on what comes.', 'I became insane, with long intervals of horrible sanity.', 'Failure is a detour, not a dead-end street.', 'I am not a product of my circumstances. I am a product of my decisions.', 'You can cut all the flowers but you cannot keep spring from coming.', \"Life doesn't run away from nobody. Life runs at people.\", 'In order to succeed, your desire for success should be greater than your fear of failure.', \"It's difficult to think anything but pleasant thoughts while eating a homegrown tomato.\", 'Too many of us are not living our dreams because we are living our fears.', 'If the only prayer you ever say in your entire life is thank you, it will be enough.', \"It's just a job. Grass grows, birds fly, waves pound the sand. I beat people up.\", 'A man must be big enough to admit his mistakes, smart enough to profit from them, and strong enough to correct them.', 'Our greatest weakness lies in giving up. The most certain way to succeed is always to try just one more time.', 'This is one small step for a man, one giant leap for mankind.', \"It has been said, 'time heals all wounds.' I do not agree. The wounds remain. In time, the mind, protecting its sanity, covers them with scar tissue and the pain lessens. But it is never gone.\", \"I have held many things in my hands, and I have lost them all; but whatever I have placed in God's hands, that I still possess.\", \"Welfare's purpose should be to eliminate, as far as possible, the need for its own existence.\", 'Each time, before you intercede, be quiet first, and worship God in His glory. Think of what He can do, and how He delights to hear the prayers of His redeemed people. Think of your place and privilege in Christ, and expect great things!', \"Failure is not the opposite of success; it's part of success.\", 'No one has ever become poor by giving.', 'It is not my ability, but my response to God’s ability, that counts.', 'No one is born hating another person because of the color of his skin, or his background, or his religion. People must learn to hate, and if they can learn to hate, they can be taught to love, for love comes more naturally to the human heart than its opposite.', 'No matter what people tell you, words and ideas can change the world.', 'A man should never neglect his family for business.', 'Those who dare to fail miserably can achieve greatly.', 'The two most important days in your life are the day you are born and the day you find out why.', 'We must be global Christians with a global vision because our God is a global God.', 'With everything that has happened to you, you can either feel sorry for yourself or treat what has happened as a gift. Everything is either an opportunity to grow or an obstacle to keep you from growing. You get to choose.', 'Don’t let the noise of others’ opinions drown out your own inner voice.', 'History, despite its wrenching pain, cannot be unlived, but if faced with courage, need not be lived again.', \"How old would you be if you didn't know how old you are?\", 'Leadership is a privilege to better the lives of others. It is not an opportunity to satisfy personal greed.', 'If you have good thoughts they will shine out of your face like sunbeams and you will always look lovely.', 'Never cut a tree down in the wintertime. Never make a negative decision in the low time. Never make your most important decisions when you are in your worst moods. Wait. Be patient. The storm will pass. The spring will come.', 'An optimist stays up until midnight to see the new year in. A pessimist stays up to make sure the old year leaves.', 'When the going gets weird, the weird turn pro.', \"When a train goes through a tunnel and it gets dark, you don't throw away the ticket and jump off. You sit still and trust the engineer.\", 'If you think you are too small to make a difference, try sleeping with a mosquito.', \"God doesn't require us to succeed, he only requires that you try.\", 'Change your thoughts and you change your world.']\n"
     ]
    }
   ],
   "source": [
    "Quote_tags = driver.find_elements(By.XPATH, '//a[@class=\"title\"]')  # Adjusted XPath to target only the rating element\n",
    "Quote_tags_list = []\n",
    "\n",
    "for tag in Quote_tags:\n",
    "    quote = tag.text  # Splitting the text and extracting the rating\n",
    "    Quote_tags_list.append(quote)\n",
    "\n",
    "print(Quote_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "d0b1cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(Quote_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "4d591cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michael Porter', 'Golda Meir', 'Theodore Roosevelt', 'Nelson Mandela', 'Erma Bombeck', 'John Wooden', 'Albert Einstein', 'Frank Zappa', 'William Faulkner', 'Will Rogers', 'Jimmy Carter', 'Albert Einstein', 'Abraham Lincoln', 'Steven Weinberg', 'Franklin D. Roosevelt', 'Albert Einstein', 'Maya Angelou', 'Mark Twain', 'Theodore Roosevelt', 'Michael Jackson', 'Martin Luther King, Jr.', 'C. S. Lewis', 'Theodore Roosevelt', 'Mark Twain', 'Abraham Lincoln', 'Ruth Graham', 'C. S. Lewis', 'Groucho Marx', 'John Lennon', 'Barbara Bush', 'Albert Einstein', 'Francis Chan', 'Henry Ford', 'Victor Hugo', 'Abu Bakr', 'Zig Ziglar', 'Winston Churchill', 'Zig Ziglar', 'John F. Kennedy', 'Dwight D. Eisenhower', 'Winston Churchill', 'Albert Einstein', 'Willie Nelson', 'Johnny Cash', 'Marie Curie', 'Maya Angelou', 'Amy Bloom', 'Zig Ziglar', 'John Lennon', 'Thomas Sowell', 'Max Lucado', 'Martin Luther King, Jr.', 'Thomas Sowell', 'Salma Hayek', 'Martin Luther King, Jr.', 'Douglas MacArthur', 'Douglas MacArthur', 'George S. Patton', 'Corrie Ten Boom', 'Jim Courier', 'Zig Ziglar', 'Edgar Allan Poe', 'Zig Ziglar', 'Stephen Covey', 'Pablo Neruda', 'Joe Frazier', 'Bill Cosby', 'Lewis Grizzard', 'Les Brown', 'Meister Eckhart', 'Muhammad Ali', 'John C. Maxwell', 'Thomas A. Edison', 'Neil Armstrong', 'Rose Kennedy', 'Martin Luther', 'Ronald Reagan', 'Andrew Murray', 'Arianna Huffington', 'Anne Frank', 'Corrie Ten Boom', 'Nelson Mandela', 'Robin Williams', 'Walt Disney', 'John F. Kennedy', 'Mark Twain', 'John Stott', 'Wayne Dyer', 'Steve Jobs', 'Maya Angelou', 'Satchel Paige', 'Mwai Kibaki', 'Roald Dahl', 'Robert H. Schuller', 'Bill Vaughan', 'Hunter S. Thompson', 'Corrie Ten Boom', 'Dalai Lama', 'Mother Teresa', 'Norman Vincent Peale']\n"
     ]
    }
   ],
   "source": [
    "Author_tags = driver.find_elements(By.XPATH, '//div[@class=\"author\"]/a')  # Adjusted XPath to target only the rating element\n",
    "Author_tags_list = []\n",
    "\n",
    "for tag in Author_tags:\n",
    "    author = tag.text  # Splitting the text and extracting the rating\n",
    "    Author_tags_list.append(author)\n",
    "\n",
    "print(Author_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "63a4e33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(Author_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "011f6cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Essence, Deep Thought, Transcendentalism', 'Inspiration, Past, Trying', 'Country, Peace, War', 'Inspirational, Motivational, Death', '4th Of July, Food, Patriotic', 'Inspirational, Success, Basketball', 'Strong, Revenge, Intelligent', 'Inspirational, Teacher, Religious', 'Truth, Honesty, Lying', 'Funny, Reading, Learning', 'Strong, Thoughtful, Compassion', 'Love, Funny, Life', 'Freedom, Men, Democracies Have', 'God, Religious, Atheist', 'Respect, Kindness, Character', 'Inspirational, Motivational, Change', 'Inspirational, Life, Inspiring', 'Love, Inspirational, Life', 'Inspiring, Country, 4th Of July', 'Inspirational, Dream, Hate', 'Love, Inspirational, Life', 'Forgiveness, God, Christian', 'Country, Men, Squares', 'Inspirational, Relationship, Positive', 'Inspirational, Motivational, Positive', 'Love, Friendship, Relationship', 'Faith, God, Christian', 'Art, Freedom, Political Will', 'Inspirational, Life, Faith', 'Inspirational, Life, Meaningful', 'Happiness, Time, Clever', 'Inspirational, Inspiring, Success', 'Love, Inspirational, Life', 'Music, Sound And Music, Musical Life', 'Adversity, Blow, Action', 'Positive, Thinking Of You, Attitude', 'Inspirational, Funny, Life', 'Inspirational, Life, Motivational', 'Inspirational, Memorial Day, Freedom', 'Freedom, Patriotic, Vaccines', 'Philosophy, Ignorance, Democratic Socialism', 'Inspirational, Dance, Education', 'Life, Change, Positive', 'Life, Bullying, Mistake', 'Life, Nature, Children', 'Inspirational, Life, Motivational', 'Marriage, Love You, Long', 'Inspirational, Motivational, Success', 'Love, Inspirational, Life', 'Diversity, Important, Liberty', 'Inspirational, God, Faith', 'Inspirational, Motivational, Inspiring', 'Business, Stupid, Government', 'Beauty, Love Yourself, Your Beautiful', 'Inspirational, Life, Motivational', 'Peace, Military, War', 'Inspirational, Leadership, Confidence', 'Motivational, Military, War', 'Christian, Jesus, Confusing', 'Inspirational, Success, Sports', 'Inspirational, Motivational, Success', 'Inspirational, Life, Clever', 'Inspirational, Death, Failure', 'Inspirational, Life, Motivational', 'Inspiring, Hope, Spring', 'Motivational, Running, People', 'Inspirational, Motivational, Positive', 'Summer, Food, Garden', 'Inspirational, Life, Motivational', 'Inspirational, Life, Thank You', 'Funny, Sports, Jobs', 'Being Strong, Wisdom, Stay Strong', 'Inspirational, Motivational, Positive', 'Motivational, Positive, Moon', 'Time, Pain, Get Well', 'Life, God, Christian', 'President, Needs, Purpose', 'God, Faith, Christian', 'Women, Failure, Successful', 'Inspirational, Morning, Kindness', 'Hiding Place, Ability, Baby Boom', 'Love, Inspirational, Life', 'Inspirational, Change, Philosophy', 'Inspirational, Family, Inspiring', 'Inspirational, Success, Courage', 'Inspirational, Life, Motivational', 'Christian, Vision, Missionary', 'Inspirational, Positive, Regret', 'Love, Inspirational, Life', 'Inspirational, Life, Encouragement', 'Happy Birthday, Baseball, Fun', 'Leadership, Power, Opportunity', 'Inspirational, Life, Positive', 'Inspirational, Encouraging, Patience', 'New Year, Fun, New Beginnings', 'Music, Sports, Hunting', 'Trust, Encouraging, Uplifting', 'Inspirational, Funny, Change', 'Success, God, Mother', 'Inspirational, Motivational, Change']\n"
     ]
    }
   ],
   "source": [
    "Type_Of_Quotes_tags = driver.find_elements(By.XPATH, '//div[@class=\"tags\"]')  # Adjusted XPath to target only the rating element\n",
    "Type_Of_Quotes_tags_list = []\n",
    "\n",
    "for tag in Type_Of_Quotes_tags:\n",
    "    type = tag.text  # Splitting the text and extracting the rating\n",
    "    Type_Of_Quotes_tags_list.append(type)\n",
    "\n",
    "print(Type_Of_Quotes_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "4e19f976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(Type_Of_Quotes_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e6199b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Type</th>\n",
       "      <th>Quote List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Music, Sports, Hunting</td>\n",
       "      <td>When the going gets weird, the weird turn pro.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Corrie Ten Boom</td>\n",
       "      <td>Trust, Encouraging, Uplifting</td>\n",
       "      <td>When a train goes through a tunnel and it gets...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Dalai Lama</td>\n",
       "      <td>Inspirational, Funny, Change</td>\n",
       "      <td>If you think you are too small to make a diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Mother Teresa</td>\n",
       "      <td>Success, God, Mother</td>\n",
       "      <td>God doesn't require us to succeed, he only req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Norman Vincent Peale</td>\n",
       "      <td>Inspirational, Motivational, Change</td>\n",
       "      <td>Change your thoughts and you change your world.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Author                                      Type  \\\n",
       "0         Michael Porter  Essence, Deep Thought, Transcendentalism   \n",
       "1             Golda Meir                 Inspiration, Past, Trying   \n",
       "2     Theodore Roosevelt                       Country, Peace, War   \n",
       "3         Nelson Mandela        Inspirational, Motivational, Death   \n",
       "4           Erma Bombeck              4th Of July, Food, Patriotic   \n",
       "..                   ...                                       ...   \n",
       "95    Hunter S. Thompson                    Music, Sports, Hunting   \n",
       "96       Corrie Ten Boom             Trust, Encouraging, Uplifting   \n",
       "97            Dalai Lama              Inspirational, Funny, Change   \n",
       "98         Mother Teresa                      Success, God, Mother   \n",
       "99  Norman Vincent Peale       Inspirational, Motivational, Change   \n",
       "\n",
       "                                           Quote List  \n",
       "0   The essence of strategy is choosing what not t...  \n",
       "1   One cannot and must not try to erase the past ...  \n",
       "2   Patriotism means to stand by the country. It d...  \n",
       "3   Death is something inevitable. When a man has ...  \n",
       "4   You have to love a nation that celebrates its ...  \n",
       "..                                                ...  \n",
       "95     When the going gets weird, the weird turn pro.  \n",
       "96  When a train goes through a tunnel and it gets...  \n",
       "97  If you think you are too small to make a diffe...  \n",
       "98  God doesn't require us to succeed, he only req...  \n",
       "99    Change your thoughts and you change your world.  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Author':Author_tags_list,'Type':Type_Of_Quotes_tags_list,'Quote List':Quote_tags_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "53eaad80",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead,\n",
    "Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0752ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening flipcart Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.jagranjosh.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05dc905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Button_GK = driver.find_element(By.XPATH,\"/html/body/div/header/nav/div/div/div[3]/ul\")\n",
    "\n",
    "Button_GK.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c507b88",
   "metadata": {},
   "source": [
    "I cant found required field for scap the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e3284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3163a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e.\n",
    "Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "603b7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening flipcart Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.motor1.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0fe409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_input = driver.find_element(By.XPATH, \"/html/body/div[10]/div[2]/div/div/div[3]/div/div/div/form/input\")\n",
    "\n",
    "location_input.send_keys('50 most expensive cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bc7234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now inspect search button find the class name & click button\n",
    "\n",
    "search = driver.find_element(By.XPATH,\"/html/body/div[10]/div[2]/div/div/div[3]/div/div/div/form/button[1]\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7af754a",
   "metadata": {},
   "source": [
    "Car_Name_tags = driver.find_elements(By.XPATH, '//h3[@class=\"subheader\"]')  # Adjusted XPath to target only the rating element\n",
    "Car_Name_tags_list = []\n",
    "\n",
    "for tag in Car_Name_tags:\n",
    "    car = tag.text  # Splitting the text and extracting the rating\n",
    "    Car_Name_tags_list.append(car)\n",
    "\n",
    "print(Car_Name_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3dda065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aston Martin Valour', 'McLaren Elva', 'Czinger 21C', 'Ferrari Monza', 'Gordon Murray T.33', 'Koenigsegg Gemera', 'Zenvo TSR-S', 'Hennessey Venom F5', 'Bentley Bacalar', 'Hispano Suiza Carmen Boulogne', 'Bentley Mulliner Batur', 'Deus Vayanne', 'SSC Tuatara', 'Lotus Evija', 'Aston Martin Vulcan', 'Delage D12', 'Ferrari Daytona SP3', 'McLaren Speedtail', 'Rimac Nevera', 'Pagani Utopia', 'Pininfarina Battista', 'Gordon Murray T.50', 'Lamborghini Countach', 'Mercedes-AMG Project One', 'Zenvo Aurora', 'Aston Martin Victor', 'Hennessey Venom F5 Roadster', 'Koenigsegg Jesko', 'Aston Martin Valkyrie', 'W Motors Lykan Hypersport', 'McLaren Solus', 'Lamborghini Sian', 'Koenigsegg CC850', 'Bugatti Chiron Super Sport 300+', 'Lamborghini Veneno', 'Bugatti Bolide', 'Pininfarina B95 Speedster', 'Bugatti Mistral', 'Pagani Huayra Imola', 'Bugatti Divo', 'SP Automotive Chaos', 'Pagani Codalunga', '777 Hypercar', 'Mercedes-Maybach Exelero', 'Bugatti Centodieci', 'Bugatti Chiron Profilée', 'Rolls-Royce Sweptail', 'Bugatti La Voiture Noire', 'Rolls-Royce Boat Tail*', 'Rolls-Royce La Rose Noire Droptail']\n"
     ]
    }
   ],
   "source": [
    "Car_Name_tags = driver.find_elements(By.XPATH, '//h3[@class=\"subheader\"]')  # Adjusted XPath to target only the rating element\n",
    "Car_Name_tags_list = []\n",
    "\n",
    "# Loop through the first 50 records only\n",
    "for i in range(50):\n",
    "    car = Car_Name_tags[i].text  # Splitting the text and extracting the rating\n",
    "    Car_Name_tags_list.append(car)\n",
    "\n",
    "print(Car_Name_tags_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b614e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_field = driver.find_element(By.XPATH,\"/html/body/div[10]/div[9]/div/div[1]/div/div/div[2]/div/div[1]\")\n",
    "\n",
    "required_field.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7af6fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Car_Name_tags_list.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e79736",
   "metadata": {},
   "source": [
    "Price_tags = driver.find_elements(By.XPATH, '//div[@class=\"postBody description e-content\"]/p/strong')  # Adjusted XPath to target only the rating element\n",
    "Price_tags_list = []\n",
    "\n",
    "for tag in Price_tags:\n",
    "    price = tag.text  # Splitting the text and extracting the rating\n",
    "    Price_tags_list.append(price)\n",
    "\n",
    "print(Price_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7c60c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Price: $1.5 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.7 Million', 'Price: $1.8 Million', 'Price: $1.9 Million', 'Price: $1.9 Million', 'Price: $2.0 Million', 'Price: $2.0 Million', 'Price: $2.0 Million', 'Price: $2.1 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.3 Million', 'Price: $2.4 Million', 'Price: $2.5 Million', 'Price: $2.5 Million', 'Price: $2.6 Million', 'Price: $2.6 Million', 'Price: $2.7 Million', 'Price: $2.8 Million', 'Price: $3.0 Million', '$3.0 Million', 'Price: $3.0 Million', 'Price: $3.2 Million', 'Price: $3.4 Million', '$3.5 Million', 'Price: $3.6 million', 'Price: $3.7 Million', 'Price: $3.9 Million', 'Price: $4.5 Million', 'Price: $4.7 Million', 'Price: $4.8 Million', 'Price: $5.0 Million', 'Price: $5.4 Million', 'Price: $5.8 Million', 'Price: $6.4 Million', 'Price: $7.4 Million', 'Price: $7.5 Million', 'Price: $8.0 Million', 'Price: $9.0 Million', 'Price: $10.8 Million', 'Price: $12.8 Million', 'Price: $13.4 Million', 'Price: $28.0 Million (est.)', 'Price: $30 Million (est.)']\n"
     ]
    }
   ],
   "source": [
    "Price_tags = driver.find_elements(By.XPATH, '//div[@class=\"postBody description e-content\"]/p/strong')  # Adjusted XPath to target only the price element\n",
    "Price_tags_list = []\n",
    "\n",
    "# Loop through the first 50 records only\n",
    "for i in range(50):\n",
    "    price = Price_tags[i].text  # Extracting the text of the price tag\n",
    "    Price_tags_list.append(price)\n",
    "\n",
    "print(Price_tags_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f7d861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price_tags_list.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "139c7b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Car Names</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aston Martin Valour</td>\n",
       "      <td>Price: $1.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McLaren Elva</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Czinger 21C</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ferrari Monza</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gordon Murray T.33</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Koenigsegg Gemera</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Zenvo TSR-S</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hennessey Venom F5</td>\n",
       "      <td>Price: $1.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bentley Bacalar</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hispano Suiza Carmen Boulogne</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bentley Mulliner Batur</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Deus Vayanne</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SSC Tuatara</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Lotus Evija</td>\n",
       "      <td>Price: $2.1 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Aston Martin Vulcan</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Delage D12</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ferrari Daytona SP3</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>McLaren Speedtail</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rimac Nevera</td>\n",
       "      <td>Price: $2.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pagani Utopia</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pininfarina Battista</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Gordon Murray T.50</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Lamborghini Countach</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mercedes-AMG Project One</td>\n",
       "      <td>Price: $2.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Zenvo Aurora</td>\n",
       "      <td>Price: $2.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Aston Martin Victor</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hennessey Venom F5 Roadster</td>\n",
       "      <td>$3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Koenigsegg Jesko</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Aston Martin Valkyrie</td>\n",
       "      <td>Price: $3.2 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>W Motors Lykan Hypersport</td>\n",
       "      <td>Price: $3.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>McLaren Solus</td>\n",
       "      <td>$3.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Lamborghini Sian</td>\n",
       "      <td>Price: $3.6 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Koenigsegg CC850</td>\n",
       "      <td>Price: $3.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Bugatti Chiron Super Sport 300+</td>\n",
       "      <td>Price: $3.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Lamborghini Veneno</td>\n",
       "      <td>Price: $4.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Bugatti Bolide</td>\n",
       "      <td>Price: $4.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Pininfarina B95 Speedster</td>\n",
       "      <td>Price: $4.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bugatti Mistral</td>\n",
       "      <td>Price: $5.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Pagani Huayra Imola</td>\n",
       "      <td>Price: $5.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Bugatti Divo</td>\n",
       "      <td>Price: $5.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>SP Automotive Chaos</td>\n",
       "      <td>Price: $6.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Pagani Codalunga</td>\n",
       "      <td>Price: $7.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>777 Hypercar</td>\n",
       "      <td>Price: $7.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Mercedes-Maybach Exelero</td>\n",
       "      <td>Price: $8.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bugatti Centodieci</td>\n",
       "      <td>Price: $9.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bugatti Chiron Profilée</td>\n",
       "      <td>Price: $10.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Rolls-Royce Sweptail</td>\n",
       "      <td>Price: $12.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Bugatti La Voiture Noire</td>\n",
       "      <td>Price: $13.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Rolls-Royce Boat Tail*</td>\n",
       "      <td>Price: $28.0 Million (est.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Rolls-Royce La Rose Noire Droptail</td>\n",
       "      <td>Price: $30 Million (est.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Car Names                        Price\n",
       "0                  Aston Martin Valour          Price: $1.5 Million\n",
       "1                         McLaren Elva          Price: $1.7 Million\n",
       "2                          Czinger 21C          Price: $1.7 Million\n",
       "3                        Ferrari Monza          Price: $1.7 Million\n",
       "4                   Gordon Murray T.33          Price: $1.7 Million\n",
       "5                    Koenigsegg Gemera          Price: $1.7 Million\n",
       "6                          Zenvo TSR-S          Price: $1.7 Million\n",
       "7                   Hennessey Venom F5          Price: $1.8 Million\n",
       "8                      Bentley Bacalar          Price: $1.9 Million\n",
       "9        Hispano Suiza Carmen Boulogne          Price: $1.9 Million\n",
       "10              Bentley Mulliner Batur          Price: $2.0 Million\n",
       "11                        Deus Vayanne          Price: $2.0 Million\n",
       "12                         SSC Tuatara          Price: $2.0 Million\n",
       "13                         Lotus Evija          Price: $2.1 Million\n",
       "14                 Aston Martin Vulcan          Price: $2.3 Million\n",
       "15                          Delage D12          Price: $2.3 Million\n",
       "16                 Ferrari Daytona SP3          Price: $2.3 Million\n",
       "17                   McLaren Speedtail          Price: $2.3 Million\n",
       "18                        Rimac Nevera          Price: $2.4 Million\n",
       "19                       Pagani Utopia          Price: $2.5 Million\n",
       "20                Pininfarina Battista          Price: $2.5 Million\n",
       "21                  Gordon Murray T.50          Price: $2.6 Million\n",
       "22                Lamborghini Countach          Price: $2.6 Million\n",
       "23            Mercedes-AMG Project One          Price: $2.7 Million\n",
       "24                        Zenvo Aurora          Price: $2.8 Million\n",
       "25                 Aston Martin Victor          Price: $3.0 Million\n",
       "26         Hennessey Venom F5 Roadster                 $3.0 Million\n",
       "27                    Koenigsegg Jesko          Price: $3.0 Million\n",
       "28               Aston Martin Valkyrie          Price: $3.2 Million\n",
       "29           W Motors Lykan Hypersport          Price: $3.4 Million\n",
       "30                       McLaren Solus                 $3.5 Million\n",
       "31                    Lamborghini Sian          Price: $3.6 million\n",
       "32                    Koenigsegg CC850          Price: $3.7 Million\n",
       "33     Bugatti Chiron Super Sport 300+          Price: $3.9 Million\n",
       "34                  Lamborghini Veneno          Price: $4.5 Million\n",
       "35                      Bugatti Bolide          Price: $4.7 Million\n",
       "36           Pininfarina B95 Speedster          Price: $4.8 Million\n",
       "37                     Bugatti Mistral          Price: $5.0 Million\n",
       "38                 Pagani Huayra Imola          Price: $5.4 Million\n",
       "39                        Bugatti Divo          Price: $5.8 Million\n",
       "40                 SP Automotive Chaos          Price: $6.4 Million\n",
       "41                    Pagani Codalunga          Price: $7.4 Million\n",
       "42                        777 Hypercar          Price: $7.5 Million\n",
       "43            Mercedes-Maybach Exelero          Price: $8.0 Million\n",
       "44                  Bugatti Centodieci          Price: $9.0 Million\n",
       "45             Bugatti Chiron Profilée         Price: $10.8 Million\n",
       "46                Rolls-Royce Sweptail         Price: $12.8 Million\n",
       "47            Bugatti La Voiture Noire         Price: $13.4 Million\n",
       "48              Rolls-Royce Boat Tail*  Price: $28.0 Million (est.)\n",
       "49  Rolls-Royce La Rose Noire Droptail    Price: $30 Million (est.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Car Names':Car_Name_tags_list,'Price':Price_tags_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b728b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f426915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb5a73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61abc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
