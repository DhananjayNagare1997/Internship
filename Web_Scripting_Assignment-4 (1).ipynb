{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c87d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86bea0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: \n",
    "A) Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa90b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36d1db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Baby_Shark', 'https://en.wikipedia.org/wiki/Pinkfong', 'https://en.wikipedia.org/wiki/Despacito#Music_video', 'https://en.wikipedia.org/wiki/Luis_Fonsi', 'https://en.wikipedia.org/wiki/Johny_Johny_Yes_Papa#YouTube_videos', 'https://en.wikipedia.org/wiki/LooLoo_Kids', 'https://www.youtube.com/watch?v=WRVsOCh907o', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Shape_of_You#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/See_You_Again#Music_video', 'https://en.wikipedia.org/wiki/Wiz_Khalifa', 'https://en.wikipedia.org/wiki/The_Wheels_on_the_Bus', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://www.youtube.com/watch?v=hq3yfQnllfQ', 'https://en.wikipedia.org/wiki/ChuChu_TV', 'https://en.wikipedia.org/wiki/Uptown_Funk#Music_video', 'https://en.wikipedia.org/wiki/Mark_Ronson', 'https://www.youtube.com/watch?v=_nAu9D-8srA&ab_channel=%D0%9C%D0%B8%D1%80%D0%BE%D1%88%D0%BA%D0%B0%D0%A2%D0%92', 'https://www.youtube.com/@miroska-tv', 'https://en.wikipedia.org/wiki/Gangnam_Style_(music_video)', 'https://en.wikipedia.org/wiki/Psy', 'https://en.wikipedia.org/wiki/Get_Movies', 'https://en.wikipedia.org/wiki/Dame_Tu_Cosita#Music_video', 'https://en.wikipedia.org/wiki/El_Chombo', 'https://en.wikipedia.org/wiki/Axel_F#Crazy_Frog_version', 'https://en.wikipedia.org/wiki/Crazy_Frog', 'https://en.wikipedia.org/wiki/Sugar_(Maroon_5_song)#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Counting_Stars#Music_video', 'https://en.wikipedia.org/wiki/OneRepublic', 'https://en.wikipedia.org/wiki/Baa,_Baa,_Black_Sheep', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Roar_(song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Masoom_(1983_film)', 'https://en.wikipedia.org/wiki/Waka_Waka_(This_Time_for_Africa)#Music_video', 'https://en.wikipedia.org/wiki/Shakira', 'https://en.wikipedia.org/wiki/Sorry_(Justin_Bieber_song)#Music_videos', 'https://en.wikipedia.org/wiki/Justin_Bieber', 'https://en.wikipedia.org/wiki/Thinking_Out_Loud#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Humpty_Dumpty', 'https://en.wikipedia.org/wiki/Hanuman_Chalisa', 'https://en.wikipedia.org/wiki/T-Series_(company)', 'https://en.wikipedia.org/wiki/Dark_Horse_(Katy_Perry_song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Perfect_(Ed_Sheeran_song)#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Let_Her_Go#Music_video', 'https://en.wikipedia.org/wiki/Passenger_(singer)', 'https://en.wikipedia.org/wiki/Faded_(Alan_Walker_song)#Music_video', 'https://en.wikipedia.org/wiki/Alan_Walker_(music_producer)', 'https://en.wikipedia.org/wiki/Girls_Like_You#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Lean_On#Music_video', 'https://en.wikipedia.org/wiki/Major_Lazer', 'https://en.wikipedia.org/wiki/Baby_Shark', 'https://en.wikipedia.org/wiki/Pinkfong', 'https://en.wikipedia.org/wiki/Despacito#Music_video', 'https://en.wikipedia.org/wiki/Luis_Fonsi', 'https://en.wikipedia.org/wiki/Johny_Johny_Yes_Papa#YouTube_videos', 'https://en.wikipedia.org/wiki/LooLoo_Kids', 'https://www.youtube.com/watch?v=WRVsOCh907o', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Shape_of_You#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/See_You_Again#Music_video', 'https://en.wikipedia.org/wiki/Wiz_Khalifa', 'https://en.wikipedia.org/wiki/The_Wheels_on_the_Bus', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://www.youtube.com/watch?v=hq3yfQnllfQ', 'https://en.wikipedia.org/wiki/ChuChu_TV', 'https://en.wikipedia.org/wiki/Uptown_Funk#Music_video', 'https://en.wikipedia.org/wiki/Mark_Ronson', 'https://www.youtube.com/watch?v=_nAu9D-8srA&ab_channel=%D0%9C%D0%B8%D1%80%D0%BE%D1%88%D0%BA%D0%B0%D0%A2%D0%92', 'https://www.youtube.com/@miroska-tv', 'https://en.wikipedia.org/wiki/Gangnam_Style_(music_video)', 'https://en.wikipedia.org/wiki/Psy', 'https://en.wikipedia.org/wiki/Get_Movies', 'https://en.wikipedia.org/wiki/Dame_Tu_Cosita#Music_video', 'https://en.wikipedia.org/wiki/El_Chombo', 'https://en.wikipedia.org/wiki/Axel_F#Crazy_Frog_version', 'https://en.wikipedia.org/wiki/Crazy_Frog', 'https://en.wikipedia.org/wiki/Sugar_(Maroon_5_song)#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Counting_Stars#Music_video', 'https://en.wikipedia.org/wiki/OneRepublic', 'https://en.wikipedia.org/wiki/Baa,_Baa,_Black_Sheep', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Roar_(song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Masoom_(1983_film)', 'https://en.wikipedia.org/wiki/Waka_Waka_(This_Time_for_Africa)#Music_video', 'https://en.wikipedia.org/wiki/Shakira', 'https://en.wikipedia.org/wiki/Sorry_(Justin_Bieber_song)#Music_videos', 'https://en.wikipedia.org/wiki/Justin_Bieber', 'https://en.wikipedia.org/wiki/Thinking_Out_Loud#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Humpty_Dumpty', 'https://en.wikipedia.org/wiki/Hanuman_Chalisa', 'https://en.wikipedia.org/wiki/T-Series_(company)', 'https://en.wikipedia.org/wiki/Dark_Horse_(Katy_Perry_song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Perfect_(Ed_Sheeran_song)#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Let_Her_Go#Music_video', 'https://en.wikipedia.org/wiki/Passenger_(singer)', 'https://en.wikipedia.org/wiki/Faded_(Alan_Walker_song)#Music_video', 'https://en.wikipedia.org/wiki/Alan_Walker_(music_producer)', 'https://en.wikipedia.org/wiki/Girls_Like_You#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Lean_On#Music_video', 'https://en.wikipedia.org/wiki/Major_Lazer', 'https://en.wikipedia.org/wiki/Baby_Shark', 'https://en.wikipedia.org/wiki/Pinkfong', 'https://en.wikipedia.org/wiki/Despacito#Music_video', 'https://en.wikipedia.org/wiki/Luis_Fonsi', 'https://en.wikipedia.org/wiki/Johny_Johny_Yes_Papa#YouTube_videos', 'https://en.wikipedia.org/wiki/LooLoo_Kids', 'https://www.youtube.com/watch?v=WRVsOCh907o', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Shape_of_You#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/See_You_Again#Music_video', 'https://en.wikipedia.org/wiki/Wiz_Khalifa', 'https://en.wikipedia.org/wiki/The_Wheels_on_the_Bus', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://www.youtube.com/watch?v=hq3yfQnllfQ', 'https://en.wikipedia.org/wiki/ChuChu_TV', 'https://en.wikipedia.org/wiki/Uptown_Funk#Music_video', 'https://en.wikipedia.org/wiki/Mark_Ronson', 'https://www.youtube.com/watch?v=_nAu9D-8srA&ab_channel=%D0%9C%D0%B8%D1%80%D0%BE%D1%88%D0%BA%D0%B0%D0%A2%D0%92', 'https://www.youtube.com/@miroska-tv', 'https://en.wikipedia.org/wiki/Gangnam_Style_(music_video)', 'https://en.wikipedia.org/wiki/Psy', 'https://en.wikipedia.org/wiki/Get_Movies', 'https://en.wikipedia.org/wiki/Dame_Tu_Cosita#Music_video', 'https://en.wikipedia.org/wiki/El_Chombo', 'https://en.wikipedia.org/wiki/Axel_F#Crazy_Frog_version', 'https://en.wikipedia.org/wiki/Crazy_Frog', 'https://en.wikipedia.org/wiki/Sugar_(Maroon_5_song)#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Counting_Stars#Music_video', 'https://en.wikipedia.org/wiki/OneRepublic', 'https://en.wikipedia.org/wiki/Baa,_Baa,_Black_Sheep', 'https://en.wikipedia.org/wiki/Cocomelon_-_Nursery_Rhymes', 'https://en.wikipedia.org/wiki/Roar_(song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Masoom_(1983_film)', 'https://en.wikipedia.org/wiki/Waka_Waka_(This_Time_for_Africa)#Music_video', 'https://en.wikipedia.org/wiki/Shakira', 'https://en.wikipedia.org/wiki/Sorry_(Justin_Bieber_song)#Music_videos', 'https://en.wikipedia.org/wiki/Justin_Bieber', 'https://en.wikipedia.org/wiki/Thinking_Out_Loud#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Humpty_Dumpty', 'https://en.wikipedia.org/wiki/Hanuman_Chalisa', 'https://en.wikipedia.org/wiki/T-Series_(company)', 'https://en.wikipedia.org/wiki/Dark_Horse_(Katy_Perry_song)#Music_video', 'https://en.wikipedia.org/wiki/Katy_Perry', 'https://en.wikipedia.org/wiki/Perfect_(Ed_Sheeran_song)#Music_video', 'https://en.wikipedia.org/wiki/Ed_Sheeran', 'https://en.wikipedia.org/wiki/Let_Her_Go#Music_video', 'https://en.wikipedia.org/wiki/Passenger_(singer)', 'https://en.wikipedia.org/wiki/Faded_(Alan_Walker_song)#Music_video', 'https://en.wikipedia.org/wiki/Alan_Walker_(music_producer)', 'https://en.wikipedia.org/wiki/Girls_Like_You#Music_video', 'https://en.wikipedia.org/wiki/Maroon_5', 'https://en.wikipedia.org/wiki/Lean_On#Music_video', 'https://en.wikipedia.org/wiki/Major_Lazer']\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "# scrape all product urls\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "for page in range(start, end):  # for Loop for scrapping 3 pages\n",
    "    urls = driver.find_elements(By.XPATH, '//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td/a')\n",
    "\n",
    "    for i in urls:\n",
    "        # scraping urls\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "print(product_urls)\n",
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e436888",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank_list = []\n",
    "Name_list = []\n",
    "Artist_list = []\n",
    "Upload_date_list = []"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8a8e683",
   "metadata": {},
   "source": [
    "for url in product_urls: # loop for every guitar in the list\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        rank = driver.find_element(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr')\n",
    "        Rank_list.append(rank.text)\n",
    "    except NoSuchElementException:\n",
    "        Rank_list.append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td/a')\n",
    "        Name_list.append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        Name_list.append('-')\n",
    "        \n",
    "    try:\n",
    "        artist = driver.find_element(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td/a')\n",
    "        Artist_list.append(artist.text)\n",
    "    except NoSuchElementException:\n",
    "        Artist_list.append('-')\n",
    "    \n",
    "    try:\n",
    "        return_date = driver.find_element(By.XPATH,'//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr/td')\n",
    "        Upload_date_list.append(return_date.text)\n",
    "    except NoSuchElementException:\n",
    "        Upload_date_list.append('-')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a43fda72",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m product_urls:\n\u001b[0;32m     14\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 15\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m         row \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//table[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]/tbody/tr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "# Assuming you've set up the webdriver and have a list of product_urls\n",
    "\n",
    "Rank_list = []\n",
    "Name_list = []\n",
    "Artist_list = []\n",
    "Upload_date_list = []\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        row = driver.find_element(By.XPATH, '//table[@class=\"sortable wikitable sticky-header static-row-numbers col3center col4right jquery-tablesorter\"]/tbody/tr')\n",
    "        \n",
    "        # Find rank\n",
    "        rank = row.find_element(By.XPATH, './td[1]')\n",
    "        Rank_list.append(rank.text)\n",
    "        \n",
    "        # Find name\n",
    "        name = row.find_element(By.XPATH, './td[2]/a')\n",
    "        Name_list.append(name.text)\n",
    "        \n",
    "        # Find artist\n",
    "        artist = row.find_element(By.XPATH, './td[3]/a')\n",
    "        Artist_list.append(artist.text)\n",
    "        \n",
    "        # Find upload date\n",
    "        upload_date = row.find_element(By.XPATH, './td[4]')\n",
    "        Upload_date_list.append(upload_date.text)\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        Rank_list.append('-')\n",
    "        Name_list.append('-')\n",
    "        Artist_list.append('-')\n",
    "        Upload_date_list.append('-')\n",
    "\n",
    "# Print or do further processing with the lists\n",
    "print(\"Rank List:\", Rank_list)\n",
    "print(\"Name List:\", Name_list)\n",
    "print(\"Artist List:\", Artist_list)\n",
    "print(\"Upload Date List:\", Upload_date_list)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5faa93ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n",
      "93\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(len(Rank_list))\n",
    "print(len(Name_list))\n",
    "print(len(Artist_list))\n",
    "print(len(Upload_date_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c03c484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ranck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artists</th>\n",
       "      <th>Uploade_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ranck Name Artists Uploade_Date\n",
       "0      -    -       -            -\n",
       "1      -    -       -            -\n",
       "2      -    -       -            -\n",
       "3      -    -       -            -\n",
       "4      -    -       -            -\n",
       "..   ...  ...     ...          ...\n",
       "88     -    -       -            -\n",
       "89     -    -       -            -\n",
       "90     -    -       -            -\n",
       "91     -    -       -            -\n",
       "92     -    -       -            -\n",
       "\n",
       "[93 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Ranck':Rank_list, 'Name':Name_list,'Artists':Artist_list,'Uploade_Date':Upload_date_list})\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0aaa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que2. Scrape the details team India’s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959179ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.bcci.tv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "367891bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From bcci.tv home page you have reach to the international fixture page through code.\n",
    "\n",
    "fixture_page = driver.find_element(By.XPATH,'/html/body/header/div[3]/div[2]/ul/div[1]/a[2]')\n",
    "\n",
    "fixture_page.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ab958",
   "metadata": {},
   "outputs": [],
   "source": [
    "seris = driver.find_element(By.XPATH,'/html/body/header/div[3]/div[2]/ul/div[1]/a[8]')\n",
    "\n",
    "seris.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ba38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "/html/body/header/div[3]/div[2]/ul/div[1]/a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0960a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENGLAND TOUR OF INDIA 2023-24', 'INDIA TOUR OF ZIMBABWE 2024', 'ICC MENS U19 WORLD CUP 2024', 'AFGHANISTAN TOUR OF INDIA 2023-24', 'ENGLAND LIONS TOUR OF INDIA', 'TRI-NATION UNDER-19S TOURNAMENT IN SOUTH AFRICA', 'AUSTRALIA WOMEN TOUR OF INDIA 2023-24', 'ACC U19 ASIA CUP 2023', 'INDIA TOUR OF SOUTH AFRICA 2023-24', 'ENGLAND WOMEN TOUR OF INDIA 2023-24']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "seris_tags = driver.find_elements(By.XPATH, '//div[@class=\"match-card__tourName ng-binding\"]')\n",
    "\n",
    "# Create a list to store the seris titles\n",
    "seris_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles\n",
    "for i in seris_tags[:10]:\n",
    "    title = i.text\n",
    "    seris_titles.append(title)\n",
    "\n",
    "# Output the scraped job titles\n",
    "print(seris_titles)\n",
    "\n",
    "print(len(seris_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca76d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Place_tags = driver.find_elements(By.XPATH, '//div[@class=\"match-card__tourName ng-binding\"]')\n",
    "\n",
    "# Create a list to store the seris titles\n",
    "place_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles\n",
    "for i in Place_tags[:10]:\n",
    "    title = i.text\n",
    "    place_titles.append(title)\n",
    "\n",
    "# Output the scraped job titles\n",
    "print(place_titles)\n",
    "\n",
    "print(len(place_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be80118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape all product urls\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "for page in range(start, end):  # for Loop for scrapping 3 pages\n",
    "    urls = driver.find_elements(By.XPATH, '//div[@class=\"match-card-bottom\"]/a')\n",
    "\n",
    "    for i in urls:\n",
    "        # scraping urls\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    # clicking the next button\n",
    "   # nxt_button = driver.find_element(By.XPATH, '//span[@class=\"s-pagination-strip\"]/a')\n",
    "  # nxt_button.click()\n",
    "\n",
    "    time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7546b274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e452da2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 18 18\n"
     ]
    }
   ],
   "source": [
    "print(len(date_list),len(time_list),len(place_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eeb845d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "\n",
    "date_list = []\n",
    "time_list = []\n",
    "place_list = []\n",
    "\n",
    "for url in product_urls: # loop for every guitar in the list\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        date = driver.find_element(By.XPATH, '//div[@class=\"match-date-info\"]/div')\n",
    "        date_list.append(date.text)\n",
    "    except NoSuchElementException:\n",
    "        date_list.append('-')\n",
    "        \n",
    "    try:\n",
    "        match_time = driver.find_element(By.XPATH, '//div[@class=\"match-date-info\"]/div')\n",
    "        time_list.append(match_time.text)\n",
    "    except NoSuchElementException:\n",
    "        time_list.append('-')\n",
    "        \n",
    "    try:\n",
    "        place = driver.find_element(By.XPATH, '//div[@class=\"match-place ng-scope\"]/span')\n",
    "        place_list.append(place.text)\n",
    "    except NoSuchElementException:\n",
    "        place_list.append('-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d70748e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Date Time Place\n",
       "0     -    -     -\n",
       "1     -    -     -\n",
       "2     -    -     -\n",
       "3     -    -     -\n",
       "4     -    -     -\n",
       "5     -    -     -\n",
       "6     -    -     -\n",
       "7     -    -     -\n",
       "8     -    -     -\n",
       "9     -    -     -\n",
       "10    -    -     -\n",
       "11    -    -     -\n",
       "12    -    -     -\n",
       "13    -    -     -\n",
       "14    -    -     -\n",
       "15    -    -     -\n",
       "16    -    -     -\n",
       "17    -    -     -"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Date':date_list, 'Time':time_list,'Place':place_list})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b1e78a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Seris</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENGLAND TOUR OF INDIA 2023-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INDIA TOUR OF ZIMBABWE 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ICC MENS U19 WORLD CUP 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFGHANISTAN TOUR OF INDIA 2023-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENGLAND LIONS TOUR OF INDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRI-NATION UNDER-19S TOURNAMENT IN SOUTH AFRICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AUSTRALIA WOMEN TOUR OF INDIA 2023-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ACC U19 ASIA CUP 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INDIA TOUR OF SOUTH AFRICA 2023-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENGLAND WOMEN TOUR OF INDIA 2023-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Seris\n",
       "0                    ENGLAND TOUR OF INDIA 2023-24\n",
       "1                      INDIA TOUR OF ZIMBABWE 2024\n",
       "2                      ICC MENS U19 WORLD CUP 2024\n",
       "3                AFGHANISTAN TOUR OF INDIA 2023-24\n",
       "4                      ENGLAND LIONS TOUR OF INDIA\n",
       "5  TRI-NATION UNDER-19S TOURNAMENT IN SOUTH AFRICA\n",
       "6            AUSTRALIA WOMEN TOUR OF INDIA 2023-24\n",
       "7                            ACC U19 ASIA CUP 2023\n",
       "8               INDIA TOUR OF SOUTH AFRICA 2023-24\n",
       "9              ENGLAND WOMEN TOUR OF INDIA 2023-24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Seris':seris_titles})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14e0cc",
   "metadata": {},
   "source": [
    "Que 3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50a998ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://statisticstimes.com/economy/india-statistics.php\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da312c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "India_state = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "\n",
    "India_state.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3eaa0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//td[@class=\"data1\"]')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "Rank_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Rank_tags[:33]:\n",
    "    rank = i.text\n",
    "    Rank_titles.append(rank)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(Rank_titles)\n",
    "\n",
    "print(len(Rank_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f33611d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Maharashtra', 'Tamil Nadu', 'Uttar Pradesh', 'Karnataka', 'Gujarat', 'West Bengal', 'Rajasthan', 'Madhya Pradesh', 'Andhra Pradesh', 'Telangana', 'Kerala', 'Delhi', 'Haryana', 'Odisha', 'Bihar', 'Punjab', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir-UT', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Arunachal Pradesh', 'Nagaland', 'Mizoram', 'Andaman & Nicobar Islands']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "States_tags = driver.find_elements(By.XPATH, '//td[@class=\"name\"]')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "States_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in States_tags[:33]:\n",
    "    state = i.text\n",
    "    States_titles.append(state)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(States_titles)\n",
    "\n",
    "print(len(States_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "360d1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '13.24%', '417.163', '-', '2,027,971', '2,364,514', '8.82%', '278.011', '1,453,321', '1,343,287', '2,257,575', '8.41%', '265.024', '1,304,678', '1,204,660', '2,241,368', '8.36%', '263.440', '1,326,319', '1,229,713', '-', '8.25%', '259.996', '-', '1,372,204', '1,554,992', '5.81%', '183.068', '854,023', '787,758', '1,413,620', '5.19%', '163.507']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "#Scraping GSDP [(Cr INR at 2011-12 prices) @21-22] because this option was not there GSDP(18-19)\n",
    "GSDP_tags = driver.find_elements(By.XPATH, '//td[@class=\"data\"]')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "GSDP_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in GSDP_tags[:33]:\n",
    "    gdp = i.text\n",
    "    GSDP_titles.append(gdp)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(GSDP_titles)\n",
    "\n",
    "print(len(GSDP_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2974abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '13.24%', '417.163', '-', '2,027,971', '2,364,514', '8.82%', '278.011', '1,453,321', '1,343,287', '2,257,575', '8.41%', '265.024', '1,304,678', '1,204,660', '2,241,368', '8.36%', '263.440', '1,326,319', '1,229,713', '-', '8.25%', '259.996', '-', '1,372,204', '1,554,992', '5.81%', '183.068', '854,023', '787,758', '1,413,620', '5.19%', '163.507']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "#Scraping GSDP [(Cr INR at 2011-12 prices) @21-22] because this option was not there GSDP(18-19)\n",
    "GSDP_tags_current = driver.find_elements(By.XPATH, '//td[@class=\"data\"]')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "GSDP_titles_current = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in GSDP_tags_current[:33]:\n",
    "    gdp = i.text\n",
    "    GSDP_titles_current.append(gdp)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(GSDP_titles_current)\n",
    "\n",
    "print(len(GSDP_titles_current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "902670fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Maharashtra', '-', '3,108,022', '13.24%', '417.163', '-', '2,027,971', '3', 'Uttar Pradesh', '2,257,575', '1,974,532', '8.41%', '265.024', '1,304,678', '1,204,660', '5', 'Gujarat', '-', '1,937,066', '8.25%', '259.996', '-', '1,372,204', '7', 'Rajasthan', '1,413,620', '1,218,193', '5.19%', '163.507', '799,449', '738,922', '9']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "Stares_tags_current = driver.find_elements(By.XPATH, '//tr[@class=\"odd\"]/td')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "Shares_titles_current = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Stares_tags_current[:33]:\n",
    "    gdp = i.text\n",
    "    Shares_titles_current.append(gdp)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(Shares_titles_current)\n",
    "\n",
    "print(len(Shares_titles_current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6e49478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>States</th>\n",
       "      <th>GDP</th>\n",
       "      <th>GDP Current</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>13.24%</td>\n",
       "      <td>13.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>417.163</td>\n",
       "      <td>417.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>2,027,971</td>\n",
       "      <td>2,027,971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>2,364,514</td>\n",
       "      <td>2,364,514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>8.82%</td>\n",
       "      <td>8.82%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>278.011</td>\n",
       "      <td>278.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>1,453,321</td>\n",
       "      <td>1,453,321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>1,343,287</td>\n",
       "      <td>1,343,287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>2,257,575</td>\n",
       "      <td>2,257,575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>8.41%</td>\n",
       "      <td>8.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>265.024</td>\n",
       "      <td>265.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>1,304,678</td>\n",
       "      <td>1,304,678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>1,204,660</td>\n",
       "      <td>1,204,660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>2,241,368</td>\n",
       "      <td>2,241,368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>8.36%</td>\n",
       "      <td>8.36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>263.440</td>\n",
       "      <td>263.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>1,326,319</td>\n",
       "      <td>1,326,319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>1,229,713</td>\n",
       "      <td>1,229,713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir-UT</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>8.25%</td>\n",
       "      <td>8.25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>259.996</td>\n",
       "      <td>259.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>1,372,204</td>\n",
       "      <td>1,372,204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>1,554,992</td>\n",
       "      <td>1,554,992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>5.81%</td>\n",
       "      <td>5.81%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>183.068</td>\n",
       "      <td>183.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>854,023</td>\n",
       "      <td>854,023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>787,758</td>\n",
       "      <td>787,758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>1,413,620</td>\n",
       "      <td>1,413,620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>5.19%</td>\n",
       "      <td>5.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>163.507</td>\n",
       "      <td>163.507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                     States        GDP GDP Current\n",
       "0     1                Maharashtra          -           -\n",
       "1     2                 Tamil Nadu     13.24%      13.24%\n",
       "2     3              Uttar Pradesh    417.163     417.163\n",
       "3     4                  Karnataka          -           -\n",
       "4     5                    Gujarat  2,027,971   2,027,971\n",
       "5     6                West Bengal  2,364,514   2,364,514\n",
       "6     7                  Rajasthan      8.82%       8.82%\n",
       "7     8             Madhya Pradesh    278.011     278.011\n",
       "8     9             Andhra Pradesh  1,453,321   1,453,321\n",
       "9    10                  Telangana  1,343,287   1,343,287\n",
       "10   11                     Kerala  2,257,575   2,257,575\n",
       "11   12                      Delhi      8.41%       8.41%\n",
       "12   13                    Haryana    265.024     265.024\n",
       "13   14                     Odisha  1,304,678   1,304,678\n",
       "14   15                      Bihar  1,204,660   1,204,660\n",
       "15   16                     Punjab  2,241,368   2,241,368\n",
       "16   17                      Assam      8.36%       8.36%\n",
       "17   18               Chhattisgarh    263.440     263.440\n",
       "18   19                  Jharkhand  1,326,319   1,326,319\n",
       "19   20                Uttarakhand  1,229,713   1,229,713\n",
       "20   21         Jammu & Kashmir-UT          -           -\n",
       "21   22           Himachal Pradesh      8.25%       8.25%\n",
       "22   23                        Goa    259.996     259.996\n",
       "23   24                    Tripura          -           -\n",
       "24   25                 Chandigarh  1,372,204   1,372,204\n",
       "25   26                 Puducherry  1,554,992   1,554,992\n",
       "26   27                  Meghalaya      5.81%       5.81%\n",
       "27   28                     Sikkim    183.068     183.068\n",
       "28   29                    Manipur    854,023     854,023\n",
       "29   30          Arunachal Pradesh    787,758     787,758\n",
       "30   31                   Nagaland  1,413,620   1,413,620\n",
       "31   32                    Mizoram      5.19%       5.19%\n",
       "32   33  Andaman & Nicobar Islands    163.507     163.507"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Rank':Rank_titles, 'States':States_titles,'GDP':GSDP_titles,'GDP Current':GSDP_titles_current})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0368f5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que 4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "844f964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://github.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41140d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Details visible for perform the scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c256a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board \n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c40c483",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https:/www.billboard.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b671aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_input = driver.find_element(By.XPATH, \"/html/body/div[3]/header/div/div[4]/div/div[1]/div[3]/div[1]/div/div/div/form/label/input\")\n",
    "\n",
    "search_input.send_keys('top 100 songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a451b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div[3]/header/div/div[4]/div/div[1]/div[3]/div[1]/div/div/div/form/input\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1396fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7a762e7b9d201e65a3755&_st_url=https%3A%2F%2Fwww.billboard.com%2Fcharts%2Fyear-end%2Ftop-billboard-200-albums%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7aa7e28ccbc69e88c693a&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvcategory%2Fcoverd%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7a79a64441ffd63aead8a&_st_url=https%3A%2F%2Fwww.billboard.com%2Fcharts%2Fyear-end%2Ftop-artists%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7a79b64441f196faec189&_st_url=https%3A%2F%2Fwww.billboard.com%2Fcharts%2Fyear-end%2Fbillboard-global-200%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7aa47196a67947ca5b56e&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvcategory%2Fgrowing-up-latino%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7aa7e64441f29c2aeae07&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvcategory%2Ffishing-for-answers%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7a79a28ccbc69e88c607c&_st_url=https%3A%2F%2Fwww.billboard.com%2Fcharts%2Fyear-end%2Fhot-100-songs%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7aa47196a67e3caa5bc9b&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvcategory%2Fhow-well-do-you-know-your-bandmates%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=61b7aa47196a6709b5a5b19a&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvcategory%2Fhow-it-went-down%2F&q=top%20100%20songs', 'https://api.swiftype.com/api/v1/public/analytics/pc?engine_key=9Y5W1rraVoDAe68YgJgT&doc_id=65df648264441fe96f4484f1&_st_url=https%3A%2F%2Fwww.billboard.com%2Fvideo%2Fbillboard-explains-luisa-sonzas-global-force-on-charts%2F&q=top%20100%20songs']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# scrape all product urls\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "\n",
    "for page in range(start, end):  # for Loop for scrapping 3 pages\n",
    "    urls = driver.find_elements(By.XPATH, '//div[@class=\"result-content\"]/div/a')\n",
    "\n",
    "    for i in urls:\n",
    "        # scraping urls\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "print(product_urls)\n",
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5dc6d8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5\n",
      "['One Thing At A Time', '-', 'Taylor Swift', 'Flowers', '-']\n",
      "['Morgan Wallen', '-', '-', 'Miley Cyrus', '-']\n",
      "['1\\nOne Thing At A Time\\nMorgan Wallen', '-', '1\\nTaylor Swift', '1\\nFlowers\\nMiley Cyrus', '-']\n"
     ]
    }
   ],
   "source": [
    "print(len(Song_name_list),len(Artist_name_list),len(Last_week_rank_list))\n",
    "\n",
    "print(Song_name_list)\n",
    "print(Artist_name_list)\n",
    "print(Last_week_rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c8fc591",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Loop for the first 100 records\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m product_urls[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m----> 8\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      9\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mGET, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: url})\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:345\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    343\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:302\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    300\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[0;32m    301\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(command_info[\u001b[38;5;241m0\u001b[39m], url, body\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:322\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    319\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 322\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conn\u001b[38;5;241m.\u001b[39mrequest(method, url, body\u001b[38;5;241m=\u001b[39mbody, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m    323\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, u\u001b[38;5;241m.\u001b[39mrequest_uri, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[38;5;241m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Empty lists for each category\n",
    "Song_name_list = []\n",
    "Artist_name_list = []\n",
    "Last_week_rank_list = []\n",
    "\n",
    "# Loop for the first 100 records\n",
    "for url in product_urls[:10]:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        song_name = driver.find_element(By.XPATH, '//li[@class=\"lrv-u-width-100p\"]/ul/li/h3')\n",
    "        Song_name_list.append(song_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Song_name_list.append('-')\n",
    "\n",
    "    try:\n",
    "        rank = driver.find_element(By.XPATH, '//div[@class=\"chart-results-list // u-padding-b-250\"]/div/ul')\n",
    "        Last_week_rank_list.append(rank.text)\n",
    "    except NoSuchElementException:\n",
    "        Last_week_rank_list.append('-')\n",
    "\n",
    "    try:\n",
    "        artist = driver.find_element(By.XPATH, '//li[@class=\"lrv-u-width-100p\"]/ul/li/span')\n",
    "        Artist_name_list.append(artist.text)\n",
    "    except NoSuchElementException:\n",
    "        Artist_name_list.append('-')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076ad3a9",
   "metadata": {},
   "source": [
    "Que6. Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abb43396",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "06333f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Da Vinci Code,The', 'Brown, Dan', '5,094,805', 'Transworld', 'Crime, Thriller & Adventure', '2', 'Harry Potter and the Deathly Hallows', 'Rowling, J.K.', '4,475,152', 'Bloomsbury', \"Children's Fiction\", '3', \"Harry Potter and the Philosopher's Stone\", 'Rowling, J.K.', '4,200,654', 'Bloomsbury', \"Children's Fiction\", '4', 'Harry Potter and the Order of the Phoenix', 'Rowling, J.K.', '4,179,479', 'Bloomsbury', \"Children's Fiction\", '5', 'Fifty Shades of Grey', 'James, E. L.', '3,758,936', 'Random House', 'Romance & Sagas', '6', 'Harry Potter and the Goblet of Fire', 'Rowling, J.K.', '3,583,215', 'Bloomsbury', \"Children's Fiction\", '7', 'Harry Potter and the Chamber of Secrets', 'Rowling, J.K.', '3,484,047', 'Bloomsbury', \"Children's Fiction\", '8', 'Harry Potter and the Prisoner of Azkaban', 'Rowling, J.K.', '3,377,906', 'Bloomsbury', \"Children's Fiction\", '9', 'Angels and Demons', 'Brown, Dan', '3,193,946', 'Transworld', 'Crime, Thriller & Adventure', '10', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Rowling, J.K.', '2,950,264', 'Bloomsbury', \"Children's Fiction\", '11', 'Fifty Shades Darker', 'James, E. L.', '2,479,784', 'Random House', 'Romance & Sagas', '12', 'Twilight', 'Meyer, Stephenie', '2,315,405', 'Little, Brown Book', 'Young Adult Fiction', '13', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Larsson, Stieg', '2,233,570', 'Quercus', 'Crime, Thriller & Adventure', '14', 'Fifty Shades Freed', 'James, E. L.', '2,193,928', 'Random House', 'Romance & Sagas', '15', 'Lost Symbol,The', 'Brown, Dan', '2,183,031', 'Transworld', 'Crime, Thriller & Adventure', '16', 'New Moon', 'Meyer, Stephenie', '2,152,737', 'Little, Brown Book', 'Young Adult Fiction', '17', 'Deception Point', 'Brown, Dan', '2,062,145', 'Transworld', 'Crime, Thriller & Adventure', '18', 'Eclipse', 'Meyer, Stephenie', '2,052,876', 'Little, Brown Book', 'Young Adult Fiction', '19', 'Lovely Bones,The', 'Sebold, Alice', '2,005,598', 'Pan Macmillan', 'General & Literary Fiction', '20', 'Curious Incident of the Dog in the Night-time,The', 'Haddon, Mark', '1,979,552', 'Random House', 'General & Literary Fiction', '21', 'Digital Fortress', 'Brown, Dan', '1,928,900', 'Transworld', 'Crime, Thriller & Adventure', '22', 'Short History of Nearly Everything,A', 'Bryson, Bill', '1,852,919', 'Transworld', 'Popular Science', '23', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Larsson, Stieg', '1,814,784', 'Quercus', 'Crime, Thriller & Adventure', '24', 'Breaking Dawn', 'Meyer, Stephenie', '1,787,118', 'Little, Brown Book', 'Young Adult Fiction', '25', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Carle, Eric', '1,783,535', 'Penguin', 'Picture Books', '26', 'Gruffalo,The', 'Donaldson, Julia', '1,781,269', 'Pan Macmillan', 'Picture Books', '27', \"Jamie's 30-Minute Meals\", 'Oliver, Jamie', '1,743,266', 'Penguin', 'Food & Drink: General', '28', 'Kite Runner,The', 'Hosseini, Khaled', '1,629,119', 'Bloomsbury', 'General & Literary Fiction', '29', 'One Day', 'Nicholls, David', '1,616,068', 'Hodder & Stoughton', 'General & Literary Fiction', '30', 'Thousand Splendid Suns,A', 'Hosseini, Khaled', '1,583,992', 'Bloomsbury', 'General & Literary Fiction', '31', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", 'Larsson, Stieg', '1,555,135', 'Quercus', 'Crime, Thriller & Adventure', '32', \"Time Traveler's Wife,The\", 'Niffenegger, Audrey', '1,546,886', 'Random House', 'General & Literary Fiction', '33', 'Atonement', 'McEwan, Ian', '1,539,428', 'Random House', 'General & Literary Fiction', '34', \"Bridget Jones's Diary:A Novel\", 'Fielding, Helen', '1,508,205', 'Pan Macmillan', 'General & Literary Fiction', '35', 'World According to Clarkson,The', 'Clarkson, Jeremy', '1,489,403', 'Penguin', 'Humour: Collections & General', '36', \"Captain Corelli's Mandolin\", 'Bernieres, Louis de', '1,352,318', 'Random House', 'General & Literary Fiction', '37', 'Sound of Laughter,The', 'Kay, Peter', '1,310,207', 'Random House', 'Autobiography: General', '38', 'Life of Pi', 'Martel, Yann', '1,310,176', 'Canongate', 'General & Literary Fiction', '39', 'Billy Connolly', 'Stephenson, Pamela', '1,231,957', 'HarperCollins', 'Biography: The Arts', '40', 'Child Called It,A', 'Pelzer, Dave', '1,217,712', 'Orion', 'Autobiography: General', '41', \"Gruffalo's Child,The\", 'Donaldson, Julia', '1,208,711', 'Pan Macmillan', 'Picture Books', '42', \"Angela's Ashes:A Memoir of a Childhood\", 'McCourt, Frank', '1,204,058', 'HarperCollins', 'Autobiography: General', '43', 'Birdsong', 'Faulks, Sebastian', '1,184,967', 'Random House', 'General & Literary Fiction', '44', 'Northern Lights:His Dark Materials S.', 'Pullman, Philip', '1,181,503', 'Scholastic Ltd.', 'Young Adult Fiction', '45', 'Labyrinth', 'Mosse, Kate', '1,181,093', 'Orion', 'General & Literary Fiction', '46', 'Harry Potter and the Half-blood Prince', 'Rowling, J.K.', '1,153,181', 'Bloomsbury', 'Science Fiction & Fantasy', '47', 'Help,The', 'Stockett, Kathryn', '1,132,336', 'Penguin', 'General & Literary Fiction', '48', 'Man and Boy', 'Parsons, Tony', '1,130,802', 'HarperCollins', 'General & Literary Fiction', '49', 'Memoirs of a Geisha', 'Golden, Arthur', '1,126,337', 'Random House', 'General & Literary Fiction', '50', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'McCall Smith, Alexander', '1,115,549', 'Little, Brown Book', 'Crime, Thriller & Adventure', '51', 'Island,The', 'Hislop, Victoria', '1,108,328', 'Headline', 'General & Literary Fiction', '52', 'PS, I Love You', 'Ahern, Cecelia', '1,107,379', 'HarperCollins', 'General & Literary Fiction', '53', 'You are What You Eat:The Plan That Will Change Your Life', 'McKeith, Gillian', '1,104,403', 'Penguin', 'Fitness & Diet', '54', 'Shadow of the Wind,The', 'Zafon, Carlos Ruiz', '1,092,349', 'Orion', 'General & Literary Fiction', '55', 'Tales of Beedle the Bard,The', 'Rowling, J.K.', '1,090,847', 'Bloomsbury', \"Children's Fiction\", '56', 'Broker,The', 'Grisham, John', '1,087,262', 'Random House', 'Crime, Thriller & Adventure', '57', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Atkins, Robert C.', '1,054,196', 'Random House', 'Fitness & Diet', '58', 'Subtle Knife,The:His Dark Materials S.', 'Pullman, Philip', '1,037,160', 'Scholastic Ltd.', 'Young Adult Fiction', '59', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', 'Truss, Lynne', '1,023,688', 'Profile Books Group', 'Usage & Writing Guides', '60', \"Delia's How to Cook:(Bk.1)\", 'Smith, Delia', '1,015,956', 'Random House', 'Food & Drink: General', '61', 'Chocolat', 'Harris, Joanne', '1,009,873', 'Transworld', 'General & Literary Fiction', '62', 'Boy in the Striped Pyjamas,The', 'Boyne, John', '1,004,414', 'Random House Childrens Books G', 'Young Adult Fiction', '63', \"My Sister's Keeper\", 'Picoult, Jodi', '1,003,780', 'Hodder & Stoughton', 'General & Literary Fiction', '64', 'Amber Spyglass,The:His Dark Materials S.', 'Pullman, Philip', '1,002,314', 'Scholastic Ltd.', 'Young Adult Fiction', '65', 'To Kill a Mockingbird', 'Lee, Harper', '998,213', 'Random House', 'General & Literary Fiction', '66', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Gray, John', '992,846', 'HarperCollins', 'Popular Culture & Media: General Interest', '67', 'Dear Fatty', 'French, Dawn', '986,753', 'Random House', 'Autobiography: The Arts', '68', 'Short History of Tractors in Ukrainian,A', 'Lewycka, Marina', '986,115', 'Penguin', 'General & Literary Fiction', '69', 'Hannibal', 'Harris, Thomas', '970,509', 'Random House', 'Crime, Thriller & Adventure', '70', 'Lord of the Rings,The', 'Tolkien, J. R. R.', '967,466', 'HarperCollins', 'Science Fiction & Fantasy', '71', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Moore, Michael', '963,353', 'Penguin', 'Current Affairs & Issues', '72', 'Interpretation of Murder,The', 'Rubenfeld, Jed', '962,515', 'Headline', 'Crime, Thriller & Adventure', '73', 'Sharon Osbourne Extreme:My Autobiography', 'Osbourne, Sharon', '959,496', 'Little, Brown Book', 'Autobiography: The Arts', '74', 'Alchemist,The:A Fable About Following Your Dream', 'Coelho, Paulo', '956,114', 'HarperCollins', 'General & Literary Fiction', '75', \"At My Mother's Knee ...:and Other Low Joints\", \"O'Grady, Paul\", '945,640', 'Transworld', 'Autobiography: The Arts', '76', 'Notes from a Small Island', 'Bryson, Bill', '931,312', 'Transworld', 'Travel Writing', '77', 'Return of the Naked Chef,The', 'Oliver, Jamie', '925,425', 'Penguin', 'Food & Drink: General', '78', 'Bridget Jones: The Edge of Reason', 'Fielding, Helen', '924,695', 'Pan Macmillan', 'General & Literary Fiction', '79', \"Jamie's Italy\", 'Oliver, Jamie', '906,968', 'Penguin', 'National & Regional Cuisine', '80', 'I Can Make You Thin', 'McKenna, Paul', '905,086', 'Transworld', 'Fitness & Diet', '81', 'Down Under', 'Bryson, Bill', '890,847', 'Transworld', 'Travel Writing', '82', 'Summons,The', 'Grisham, John', '869,671', 'Random House', 'Crime, Thriller & Adventure', '83', 'Small Island', 'Levy, Andrea', '869,659', 'Headline', 'General & Literary Fiction', '84', 'Nigella Express', 'Lawson, Nigella', '862,602', 'Random House', 'Food & Drink: General', '85', 'Brick Lane', 'Ali, Monica', '856,540', 'Transworld', 'General & Literary Fiction', '86', \"Memory Keeper's Daughter,The\", 'Edwards, Kim', '845,858', 'Penguin', 'General & Literary Fiction', '87', 'Room on the Broom', 'Donaldson, Julia', '842,535', 'Pan Macmillan', 'Picture Books', '88', 'About a Boy', 'Hornby, Nick', '828,215', 'Penguin', 'General & Literary Fiction', '89', 'My Booky Wook', 'Brand, Russell', '820,563', 'Hodder & Stoughton', 'Autobiography: The Arts', '90', 'God Delusion,The', 'Dawkins, Richard', '816,907', 'Transworld', 'Popular Science', '91', '\"Beano\" Annual,The', '0', '816,585', 'D.C. Thomson', \"Children's Annuals\", '92', 'White Teeth', 'Smith, Zadie', '815,586', 'Penguin', 'General & Literary Fiction', '93', 'House at Riverton,The', 'Morton, Kate', '814,370', 'Pan Macmillan', 'General & Literary Fiction', '94', 'Book Thief,The', 'Zusak, Markus', '809,641', 'Transworld', 'General & Literary Fiction', '95', 'Nights of Rain and Stars', 'Binchy, Maeve', '808,900', 'Orion', 'General & Literary Fiction', '96', 'Ghost,The', 'Harris, Robert', '807,311', 'Random House', 'General & Literary Fiction', '97', 'Happy Days with the Naked Chef', 'Oliver, Jamie', '794,201', 'Penguin', 'Food & Drink: General', '98', 'Hunger Games,The:Hunger Games Trilogy', 'Collins, Suzanne', '792,187', 'Scholastic Ltd.', 'Young Adult Fiction', '99', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", 'Pelzer, Dave', '791,507', 'Orion', 'Biography: General', '100', \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\", 'Oliver, Jamie', '791,095', 'Penguin', 'Food & Drink: General']\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//div[@class=\"embed block\"]/table/tbody/tr/td')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "States_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Rank_tags:\n",
    "    rank = i.text\n",
    "    States_titles.append(rank)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(States_titles)\n",
    "\n",
    "print(len(States_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c080828",
   "metadata": {},
   "outputs": [],
   "source": [
    "//div[@class=\"embed block\"]/table/tbody/tr/td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "317436ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(States_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "26c12b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100']\n",
      "Book Name: ['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n",
      "Author Name: ['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n",
      "Volumes Sold: ['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n",
      "Publisher: ['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n",
      "Genre: ['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//div[@class=\"embed block\"]/table/tbody/tr/td')\n",
    "\n",
    "# Create lists to store each category\n",
    "Rank = []\n",
    "Book_name = []\n",
    "Author_name = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []\n",
    "\n",
    "# Iterate through the elements and extract data for the first 100 records\n",
    "for i in range(0, len(Rank_tags), 6):\n",
    "    rank = Rank_tags[i].text\n",
    "    book_name = Rank_tags[i + 1].text\n",
    "    author_name = Rank_tags[i + 2].text\n",
    "    volumes_sold = Rank_tags[i + 3].text\n",
    "    publisher = Rank_tags[i + 4].text\n",
    "    genre = Rank_tags[i + 5].text\n",
    "\n",
    "    Rank.append(rank)\n",
    "    Book_name.append(book_name)\n",
    "    Author_name.append(author_name)\n",
    "    Volumes_sold.append(volumes_sold)\n",
    "    Publisher.append(publisher)\n",
    "    Genre.append(genre)\n",
    "\n",
    "# Output the scraped data\n",
    "print(\"Rank:\", Rank)\n",
    "print(\"Book Name:\", Book_name)\n",
    "print(\"Author Name:\", Author_name)\n",
    "print(\"Volumes Sold:\", Volumes_sold)\n",
    "print(\"Publisher:\", Publisher)\n",
    "print(\"Genre:\", Genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "673dab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Book</th>\n",
       "      <th>Author</th>\n",
       "      <th>Volumes_sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                               Book            Author  \\\n",
       "0     1                                  Da Vinci Code,The        Brown, Dan   \n",
       "1     2               Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2     3           Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3     4          Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4     5                               Fifty Shades of Grey      James, E. L.   \n",
       "..  ...                                                ...               ...   \n",
       "95   96                                          Ghost,The    Harris, Robert   \n",
       "96   97                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97   98              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98   99  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  100  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volumes_sold        Publisher                        Genre  \n",
       "0     5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152       Bloomsbury           Children's Fiction  \n",
       "2     4,200,654       Bloomsbury           Children's Fiction  \n",
       "3     4,179,479       Bloomsbury           Children's Fiction  \n",
       "4     3,758,936     Random House              Romance & Sagas  \n",
       "..          ...              ...                          ...  \n",
       "95      807,311     Random House   General & Literary Fiction  \n",
       "96      794,201          Penguin        Food & Drink: General  \n",
       "97      792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98      791,507            Orion           Biography: General  \n",
       "99      791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Rank':Rank, 'Book':Book_name,'Author':Author_name,'Volumes_sold':Volumes_sold,'Publisher':Publisher,'Genre':Genre})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18368724",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details: \n",
    "A) Name \n",
    "B) Year span \n",
    "C) Genre \n",
    "D) Run time \n",
    "E) Ratings \n",
    "F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6753f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://www.imdb.com/search/title/?title_type=tv_series&sort=num_votes,desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "97e3de0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. Game of Thrones', '2. Breaking Bad', '3. Stranger Things', '4. Friends', '5. The Walking Dead', '6. Sherlock', '7. The Big Bang Theory', '8. Dexter', '9. How I Met Your Mother', '10. The Office', '11. True Detective', '12. Peaky Blinders', '13. Better Call Saul', '14. The Boys', '15. Black Mirror', '16. Rick and Morty', '17. Lost', '18. The Mandalorian', '19. Vikings', '20. Prison Break', '21. The Witcher', '22. Squid Game', '23. Westworld', '24. House of Cards', '25. Money Heist', '26. House', '27. The Last of Us', '28. Attack on Titan', '29. Supernatural', '30. Modern Family', '31. Suits', '32. Daredevil', '33. Narcos', '34. The Sopranos', '35. Arrow', '36. Dark', '37. The Simpsons', '38. Fargo', '39. Mr. Robot', '40. Loki', '41. South Park', '42. The Wire', '43. Death Note', '44. The Flash', '45. House of the Dragon', '46. Family Guy', '47. Avatar: The Last Airbender', '48. Homeland', '49. Brooklyn Nine-Nine', '50. Wednesday', 'Recently viewed']\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//h3[@class=\"ipc-title__text\"]')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "States_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Rank_tags:\n",
    "    rank = i.text\n",
    "    States_titles.append(rank)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(States_titles)\n",
    "\n",
    "print(len(States_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51cd76b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexError occurred at index 50\n",
      "Name: ['Game of Thrones', 'Sherlock', 'True Detective', 'Rick and Morty', 'The Witcher', 'House', 'Suits', 'Dark', 'South Park', 'Family Guy']\n",
      "Year Span: ['Breaking Bad', 'The Big Bang Theory', 'Peaky Blinders', 'Lost', 'Squid Game', 'The Last of Us', 'Daredevil', 'The Simpsons', 'The Wire', 'Avatar: The Last Airbender']\n",
      "Run Time: ['Stranger Things', 'Dexter', 'Better Call Saul', 'The Mandalorian', 'Westworld', 'Attack on Titan', 'Narcos', 'Fargo', 'Death Note', 'Homeland']\n",
      "Ratings: ['Friends', 'How I Met Your Mother', 'The Boys', 'Vikings', 'House of Cards', 'Supernatural', 'The Sopranos', 'Mr. Robot', 'The Flash', 'Brooklyn Nine-Nine']\n",
      "Votes: ['The Walking Dead', 'The Office', 'Black Mirror', 'Prison Break', 'Money Heist', 'Modern Family', 'Arrow', 'Loki', 'House of the Dragon', 'Wednesday']\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//h3[@class=\"ipc-title__text\"]')\n",
    "\n",
    "# Create lists to store each category\n",
    "Name = []      # Additional list\n",
    "Year_span = [] # Additional list\n",
    "Run_time = []  # Additional list\n",
    "Ratings = []   # Additional list\n",
    "Votes = []     # Additional list\n",
    "\n",
    "# Iterate through the elements and extract data for the first 100 records\n",
    "for i in range(0, len(Rank_tags), 5):  # Update the range to match the number of columns\n",
    "    try:\n",
    "        # Extracting the text and removing the number at the beginning\n",
    "        name = Rank_tags[i].text.split('. ', 1)[1]\n",
    "        year_span = Rank_tags[i + 1].text.split('. ', 1)[1]\n",
    "        run_time = Rank_tags[i + 2].text.split('. ', 1)[1]\n",
    "        ratings = Rank_tags[i + 3].text.split('. ', 1)[1]\n",
    "        votes = Rank_tags[i + 4].text.split('. ', 1)[1]\n",
    "\n",
    "        Name.append(name)\n",
    "        Year_span.append(year_span)\n",
    "        Run_time.append(run_time)\n",
    "        Ratings.append(ratings)\n",
    "        Votes.append(votes)\n",
    "    except IndexError:\n",
    "        # Handle the case where the split doesn't produce the expected result\n",
    "        print(f\"IndexError occurred at index {i}\")\n",
    "\n",
    "# Output the scraped data\n",
    "print(\"Name:\", Name)\n",
    "print(\"Year Span:\", Year_span)\n",
    "print(\"Run Time:\", Run_time)\n",
    "print(\"Ratings:\", Ratings)\n",
    "print(\"Votes:\", Votes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76294f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year_span</th>\n",
       "      <th>Run_time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>Breaking Bad</td>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>Friends</td>\n",
       "      <td>The Walking Dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sherlock</td>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>Dexter</td>\n",
       "      <td>How I Met Your Mother</td>\n",
       "      <td>The Office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True Detective</td>\n",
       "      <td>Peaky Blinders</td>\n",
       "      <td>Better Call Saul</td>\n",
       "      <td>The Boys</td>\n",
       "      <td>Black Mirror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rick and Morty</td>\n",
       "      <td>Lost</td>\n",
       "      <td>The Mandalorian</td>\n",
       "      <td>Vikings</td>\n",
       "      <td>Prison Break</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Witcher</td>\n",
       "      <td>Squid Game</td>\n",
       "      <td>Westworld</td>\n",
       "      <td>House of Cards</td>\n",
       "      <td>Money Heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>House</td>\n",
       "      <td>The Last of Us</td>\n",
       "      <td>Attack on Titan</td>\n",
       "      <td>Supernatural</td>\n",
       "      <td>Modern Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Suits</td>\n",
       "      <td>Daredevil</td>\n",
       "      <td>Narcos</td>\n",
       "      <td>The Sopranos</td>\n",
       "      <td>Arrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dark</td>\n",
       "      <td>The Simpsons</td>\n",
       "      <td>Fargo</td>\n",
       "      <td>Mr. Robot</td>\n",
       "      <td>Loki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>South Park</td>\n",
       "      <td>The Wire</td>\n",
       "      <td>Death Note</td>\n",
       "      <td>The Flash</td>\n",
       "      <td>House of the Dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Family Guy</td>\n",
       "      <td>Avatar: The Last Airbender</td>\n",
       "      <td>Homeland</td>\n",
       "      <td>Brooklyn Nine-Nine</td>\n",
       "      <td>Wednesday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name                   Year_span          Run_time  \\\n",
       "0  Game of Thrones                Breaking Bad   Stranger Things   \n",
       "1         Sherlock         The Big Bang Theory            Dexter   \n",
       "2   True Detective              Peaky Blinders  Better Call Saul   \n",
       "3   Rick and Morty                        Lost   The Mandalorian   \n",
       "4      The Witcher                  Squid Game         Westworld   \n",
       "5            House              The Last of Us   Attack on Titan   \n",
       "6            Suits                   Daredevil            Narcos   \n",
       "7             Dark                The Simpsons             Fargo   \n",
       "8       South Park                    The Wire        Death Note   \n",
       "9       Family Guy  Avatar: The Last Airbender          Homeland   \n",
       "\n",
       "                 Ratings                Votes  \n",
       "0                Friends     The Walking Dead  \n",
       "1  How I Met Your Mother           The Office  \n",
       "2               The Boys         Black Mirror  \n",
       "3                Vikings         Prison Break  \n",
       "4         House of Cards          Money Heist  \n",
       "5           Supernatural        Modern Family  \n",
       "6           The Sopranos                Arrow  \n",
       "7              Mr. Robot                 Loki  \n",
       "8              The Flash  House of the Dragon  \n",
       "9     Brooklyn Nine-Nine            Wednesday  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'Name':Name, 'Year_span':Year_span,'Run_time':Run_time,'Ratings':Ratings,'Votes':Votes})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1437292",
   "metadata": {},
   "outputs": [],
   "source": [
    "Que8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year \n",
    " Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d7a0bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "#Opening Shine Page an autometed chrome browser.\n",
    "\n",
    "driver.get(\"https://archive.ics.uci.edu/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4a5d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/header/nav/ul/li[1]/a\")\n",
    "\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "939ce0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "required_data = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/form/div[4]/div\")\n",
    "\n",
    "required_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3abc607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris', 'Dry Bean Dataset', 'Heart Disease', 'Rice (Cammeo and Osmancik)', 'Adult', 'Raisin', 'Breast Cancer Wisconsin (Diagnostic)', 'Wine', 'Wine Quality', 'Diabetes', 'Car Evaluation', 'Bank Marketing', 'Mushroom', 'Abalone', 'Student Performance', 'Census Income', 'Online Retail', 'Automobile', 'Statlog (German Credit Data)', 'Breast Cancer', 'Auto MPG', 'Breast Cancer Wisconsin (Original)', 'Spambase', \"Predict Students' Dropout and Academic Success\", 'Glass Identification']\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "Rank_tags = driver.find_elements(By.XPATH, '//div[@class=\"flex flex-col gap-1\"]/div/div/div/h2/a')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "States_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Rank_tags:\n",
    "    rank = i.text\n",
    "    States_titles.append(rank)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(States_titles)\n",
    "\n",
    "print(len(States_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5f37325c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://archive.ics.uci.edu/dataset/53/iris', 'https://archive.ics.uci.edu/dataset/602/dry+bean+dataset', 'https://archive.ics.uci.edu/dataset/45/heart+disease', 'https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik', 'https://archive.ics.uci.edu/dataset/2/adult', 'https://archive.ics.uci.edu/dataset/850/raisin', 'https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic', 'https://archive.ics.uci.edu/dataset/109/wine', 'https://archive.ics.uci.edu/dataset/186/wine+quality', 'https://archive.ics.uci.edu/dataset/34/diabetes']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# scrape all product urls\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "\n",
    "for page in range(start, end):  # for Loop for scrapping 3 pages\n",
    "    urls = driver.find_elements(By.XPATH, '//h2[@class=\"truncate text-primary\"]/a')\n",
    "\n",
    "    for i in urls:\n",
    "        # scraping urls\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "print(product_urls)    \n",
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9a10d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Image', 'Multivariate', 'Sequential', 'Spatiotemporal', 'Tabular', 'Text', 'Time-Series', 'Other']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "Data_Types_tags = driver.find_elements(By.XPATH, '//div[@class=\"p-4 pt-0\"]/div/label')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "Data_Types_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Data_Types_tags:\n",
    "    datatype = i.text\n",
    "    Data_Types_titles.append(datatype)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(Data_Types_titles)\n",
    "\n",
    "print(len(Data_Types_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "67ac86a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_data = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/form/div[6]/div\")\n",
    "\n",
    "task_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0970f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Classification', 'Regression', 'Clustering', 'Other']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#/html/body/div/div[1]/div[1]/main/div/div[1]/form/div[6]/div/label/span\n",
    "\n",
    "Task_tags = driver.find_elements(By.XPATH, '//div[@class=\"p-4 pt-0\"]/div/label')\n",
    "\n",
    "# Create a list to store the series titles\n",
    "Task_tags_titles = []\n",
    "\n",
    "# Iterate through the elements and extract titles for the first 33 records\n",
    "for i in Task_tags:\n",
    "    task = i.text\n",
    "    Task_tags_titles.append(task)\n",
    "\n",
    "# Output the scraped series titles\n",
    "print(Task_tags_titles)\n",
    "\n",
    "print(len(Task_tags_titles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_data = driver.find_element(By.XPATH,\"/html/body/div/div[1]/div[1]/main/div/div[1]/form/div[6]/div\")\n",
    "\n",
    "task_data.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "//h2[@class=\"truncate text-primary\"]/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8cf6bfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "# scrape all product urls\n",
    "\n",
    "product_urls = []\n",
    "\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "for page in range(start, end):  # for Loop for scrapping 3 pages\n",
    "    urls = driver.find_elements(By.XPATH, '//h2[@class=\"truncate text-primary\"]/a')\n",
    "\n",
    "    for i in urls:\n",
    "        # scraping urls\n",
    "        product_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    # clicking the next button\n",
    "    nxt_button = driver.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/main/div/div[2]/div[3]/div/button[2]')\n",
    "    nxt_button.click()\n",
    "\n",
    "    time.sleep(2)\n",
    "print(len(product_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "73edc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tabular', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate', 'Multivariate', 'Multivariate, Sequential, Time-Series', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Multivariate', 'Tabular', 'Multivariate']\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "Instances_list = []\n",
    "\n",
    "# Specify the maximum number of records to scrape\n",
    "max_records = 100\n",
    "\n",
    "for index, url in enumerate(product_urls):  # loop for every guitar in the list\n",
    "    if index >= max_records:\n",
    "        break\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        instance = driver.find_element(By.XPATH, '//div[@class=\"col-span-4\"]/p')\n",
    "        Instances_list.append(instance.text)\n",
    "    except NoSuchElementException:\n",
    "        Instances_list.append('-')\n",
    "print(Instances_list)\n",
    "print(len(Instances_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a32d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33ea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
